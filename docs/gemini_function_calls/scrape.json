[
    {
  "url": "https://ai.google.dev/api/caching#Schema",
  "filter": "fit",
  "query": null,
  "cache": "0",
  "markdown": "[ Skip to main content ](https://ai.google.dev/api/caching#main-content)\n  * English\n  * Deutsch\n  * Español – América Latina\n  * Français\n  * Indonesia\n  * Italiano\n  * Português – Brasil\n  * Tiếng Việt\n  * Русский\n  * العربيّة\n  * ภาษาไทย\n  * 中文 – 简体\n  * 中文 – 繁體\n\n[ Get API key ](https://aistudio.google.com/apikey) [ Cookbook ](https://github.com/google-gemini/cookbook) [ Community ](https://discuss.ai.google.dev/c/gemini-api/) Sign in\n\n\nSend feedback \n#  Caching\nContext caching allows you to save and reuse precomputed input tokens that you wish to use repeatedly, for example when asking different questions about the same media file. This can lead to cost and speed savings, depending on the usage. For a detailed introduction, see the [Context caching](https://ai.google.dev/gemini-api/docs/caching) guide.\n## Method: cachedContents.create\n  * [Example request](https://ai.google.dev/api/caching#body.codeSnippets)\n\n\nCreates CachedContent resource.\n### Endpoint\n`https://generativelanguage.googleapis.com/v1beta/cachedContents`\n### Request body\nThe request body contains an instance of .\nFields \n`contents[]` `object ()`\nOptional. Input only. Immutable. The content to cache.\n`tools[]` `object ()`\nOptional. Input only. Immutable. A list of `Tools` the model may use to generate the next response\n`expiration` `Union type`\nSpecifies when this resource will expire. `expiration` can be only one of the following:\n`expireTime` `string ( format)`\nTimestamp in UTC of when this resource is considered expired. This is _always_ provided on output, regardless of what was sent on input.\nUses RFC 3339, where generated output will always be Z-normalized and use 0, 3, 6 or 9 fractional digits. Offsets other than \"Z\" are also accepted. Examples: `\"2014-10-02T15:01:23Z\"`, `\"2014-10-02T15:01:23.045123456Z\"` or `\"2014-10-02T15:01:23+05:30\"`.\n`string ( format)`\nInput only. New TTL for this resource, input only.\nA duration in seconds with up to nine fractional digits, ending with '`s`'. Example: `\"3.5s\"`.\n`displayName` `string`\nOptional. Immutable. The user-generated meaningful display name of the cached content. Maximum 128 Unicode characters.\n`model` `string`\nRequired. Immutable. The name of the `Model` to use for cached content Format: `models/{model}`\n`systemInstruction` `object ()`\nOptional. Input only. Immutable. Developer set system instruction. Currently text only.\n`toolConfig` `object ()`\nOptional. Input only. Immutable. Tool config. This config is shared for all tools.\n### Example request\n### Basic\n### Python\n```\nfromgoogleimport genai\nfromgoogle.genaiimport types\n\nclient = genai.Client()\ndocument = client.files.upload(file=media / \"a11.txt\")\nmodel_name = \"gemini-1.5-flash-001\"\n\ncache = client.caches.create(\n    model=model_name,\n    config=types.CreateCachedContentConfig(\n        contents=[document],\n        system_instruction=\"You are an expert analyzing transcripts.\",\n    ),\n)\nprint(cache)\n\nresponse = client.models.generate_content(\n    model=model_name,\n    contents=\"Please summarize this transcript\",\n    config=types.GenerateContentConfig(cached_content=cache.name),\n)\nprint(response.text)\n[cache.py](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/python/cache.py#L25-L46)\n\n```\n\n### Node.js\n```\n// Make sure to include the following import:\n// import {GoogleGenAI} from '@google/genai';\nconstai=newGoogleGenAI({apiKey:process.env.GEMINI_API_KEY});\nconstfilePath=path.join(media,\"a11.txt\");\nconstdocument=awaitai.files.upload({\nfile:filePath,\nconfig:{mimeType:\"text/plain\"},\n});\nconsole.log(\"Uploaded file name:\",document.name);\nconstmodelName=\"gemini-1.5-flash-001\";\n\nconstcontents=[\ncreateUserContent(createPartFromUri(document.uri,document.mimeType)),\n];\n\nconstcache=awaitai.caches.create({\nmodel:modelName,\nconfig:{\ncontents:contents,\nsystemInstruction:\"You are an expert analyzing transcripts.\",\n},\n});\nconsole.log(\"Cache created:\",cache);\n\nconstresponse=awaitai.models.generateContent({\nmodel:modelName,\ncontents:\"Please summarize this transcript\",\nconfig:{cachedContent:cache.name},\n});\nconsole.log(\"Response text:\",response.text);\n[cache.js](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/javascript/cache.js#L33-L62)\n\n```\n\n### Go\n```\nctx:=context.Background()\nclient,err:=genai.NewClient(ctx,&genai.ClientConfig{\nAPIKey:os.Getenv(\"GEMINI_API_KEY\"),\nBackend:genai.BackendGeminiAPI,\n})\niferr!=nil{\nlog.Fatal(err)\n}\n\nmodelName:=\"gemini-1.5-flash-001\"\ndocument,err:=client.Files.UploadFromPath(\nctx,\nfilepath.Join(getMedia(),\"a11.txt\"),\n&genai.UploadFileConfig{\nMIMEType:\"text/plain\",\n},\n)\niferr!=nil{\nlog.Fatal(err)\n}\nparts:=[]*genai.Part{\ngenai.NewPartFromURI(document.URI,document.MIMEType),\n}\ncontents:=[]*genai.Content{\ngenai.NewContentFromParts(parts,genai.RoleUser),\n}\ncache,err:=client.Caches.Create(ctx,modelName,&genai.CreateCachedContentConfig{\nContents:contents,\nSystemInstruction:genai.NewContentFromText(\n\"You are an expert analyzing transcripts.\",genai.RoleUser,\n),\n})\niferr!=nil{\nlog.Fatal(err)\n}\nfmt.Println(\"Cache created:\")\nfmt.Println(cache)\n\n// Use the cache for generating content.\nresponse,err:=client.Models.GenerateContent(\nctx,\nmodelName,\ngenai.Text(\"Please summarize this transcript\"),\n&genai.GenerateContentConfig{\nCachedContent:cache.Name,\n},\n)\niferr!=nil{\nlog.Fatal(err)\n}\nprintResponse(response)\n[cache.go](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/go/cache.go#L16-L66)\n\n```\n\n### Shell\n```\nwgethttps://storage.googleapis.com/generativeai-downloads/data/a11.txt\necho'{\n  \"model\": \"models/gemini-1.5-flash-001\",\n  \"contents\":[\n\n      \"parts\":[\n\n          \"inline_data\": {\n            \"mime_type\":\"text/plain\",\n            \"data\": \"'$(base64$B64FLAGSa11.txt)'\"\n\n\n\n    \"role\": \"user\"\n\n  ],\n  \"systemInstruction\": {\n    \"parts\": [\n\n        \"text\": \"You are an expert at analyzing transcripts.\"\n\n\n  },\n  \"ttl\": \"300s\"\n}'request.json\n\ncurl-XPOST\"https://generativelanguage.googleapis.com/v1beta/cachedContents?key=$GEMINI_API_KEY\"\\\n-H'Content-Type: application/json'\\\n-d@request.json\\\ncache.json\n\nCACHE_NAME=$(catcache.json|grep'\"name\":'|cut-d'\"'-f4|head-n1)\n\ncurl-XPOST\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-001:generateContent?key=$GEMINI_API_KEY\"\\\n-H'Content-Type: application/json'\\\n-d'{\n      \"contents\": [\n\n          \"parts\":[{\n            \"text\": \"Please summarize this transcript\"\n          }],\n          \"role\": \"user\"\n\n\n      \"cachedContent\": \"'$CACHE_NAME'\"\n    }'\n```\n\n### From name\n### Python\n```\nfromgoogleimport genai\nfromgoogle.genaiimport types\n\nclient = genai.Client()\ndocument = client.files.upload(file=media / \"a11.txt\")\nmodel_name = \"gemini-1.5-flash-001\"\n\ncache = client.caches.create(\n    model=model_name,\n    config=types.CreateCachedContentConfig(\n        contents=[document],\n        system_instruction=\"You are an expert analyzing transcripts.\",\n    ),\n)\ncache_name = cache.name  # Save the name for later\n\n# Later retrieve the cache\ncache = client.caches.get(name=cache_name)\nresponse = client.models.generate_content(\n    model=model_name,\n    contents=\"Find a lighthearted moment from this transcript\",\n    config=types.GenerateContentConfig(cached_content=cache.name),\n)\nprint(response.text)\n[cache.py](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/python/cache.py#L52-L75)\n\n```\n\n### Node.js\n```\n// Make sure to include the following import:\n// import {GoogleGenAI} from '@google/genai';\nconstai=newGoogleGenAI({apiKey:process.env.GEMINI_API_KEY});\nconstfilePath=path.join(media,\"a11.txt\");\nconstdocument=awaitai.files.upload({\nfile:filePath,\nconfig:{mimeType:\"text/plain\"},\n});\nconsole.log(\"Uploaded file name:\",document.name);\nconstmodelName=\"gemini-1.5-flash-001\";\n\nconstcontents=[\ncreateUserContent(createPartFromUri(document.uri,document.mimeType)),\n];\n\nconstcache=awaitai.caches.create({\nmodel:modelName,\nconfig:{\ncontents:contents,\nsystemInstruction:\"You are an expert analyzing transcripts.\",\n},\n});\nconstcacheName=cache.name;// Save the name for later\n\n// Later retrieve the cache\nconstretrievedCache=awaitai.caches.get({name:cacheName});\nconstresponse=awaitai.models.generateContent({\nmodel:modelName,\ncontents:\"Find a lighthearted moment from this transcript\",\nconfig:{cachedContent:retrievedCache.name},\n});\nconsole.log(\"Response text:\",response.text);\n[cache.js](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/javascript/cache.js#L71-L102)\n\n```\n\n### Go\n```\nctx:=context.Background()\nclient,err:=genai.NewClient(ctx,&genai.ClientConfig{\nAPIKey:os.Getenv(\"GEMINI_API_KEY\"),\nBackend:genai.BackendGeminiAPI,\n})\niferr!=nil{\nlog.Fatal(err)\n}\n\nmodelName:=\"gemini-1.5-flash-001\"\ndocument,err:=client.Files.UploadFromPath(\nctx,\nfilepath.Join(getMedia(),\"a11.txt\"),\n&genai.UploadFileConfig{\nMIMEType:\"text/plain\",\n},\n)\niferr!=nil{\nlog.Fatal(err)\n}\nparts:=[]*genai.Part{\ngenai.NewPartFromURI(document.URI,document.MIMEType),\n}\ncontents:=[]*genai.Content{\ngenai.NewContentFromParts(parts,genai.RoleUser),\n}\ncache,err:=client.Caches.Create(ctx,modelName,&genai.CreateCachedContentConfig{\nContents:contents,\nSystemInstruction:genai.NewContentFromText(\n\"You are an expert analyzing transcripts.\",genai.RoleUser,\n),\n})\niferr!=nil{\nlog.Fatal(err)\n}\ncacheName:=cache.Name\n\n// Later retrieve the cache.\ncache,err=client.Caches.Get(ctx,cacheName,&genai.GetCachedContentConfig{})\niferr!=nil{\nlog.Fatal(err)\n}\n\nresponse,err:=client.Models.GenerateContent(\nctx,\nmodelName,\ngenai.Text(\"Find a lighthearted moment from this transcript\"),\n&genai.GenerateContentConfig{\nCachedContent:cache.Name,\n},\n)\niferr!=nil{\nlog.Fatal(err)\n}\nfmt.Println(\"Response from cache (create from name):\")\nprintResponse(response)\n[cache.go](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/go/cache.go#L76-L131)\n\n```\n\n### From chat\n### Python\n```\nfromgoogleimport genai\nfromgoogle.genaiimport types\n\nclient = genai.Client()\nmodel_name = \"gemini-1.5-flash-001\"\nsystem_instruction = \"You are an expert analyzing transcripts.\"\n\n# Create a chat session with the given system instruction.\nchat = client.chats.create(\n    model=model_name,\n    config=types.GenerateContentConfig(system_instruction=system_instruction),\n)\ndocument = client.files.upload(file=media / \"a11.txt\")\n\nresponse = chat.send_message(\n    message=[\"Hi, could you summarize this transcript?\", document]\n)\nprint(\"\\n\\nmodel:  \", response.text)\nresponse = chat.send_message(\n    message=[\"Okay, could you tell me more about the trans-lunar injection\"]\n)\nprint(\"\\n\\nmodel:  \", response.text)\n\n# To cache the conversation so far, pass the chat history as the list of contents.\ncache = client.caches.create(\n    model=model_name,\n    config={\n        \"contents\": chat.get_history(),\n        \"system_instruction\": system_instruction,\n    },\n)\n# Continue the conversation using the cached content.\nchat = client.chats.create(\n    model=model_name,\n    config=types.GenerateContentConfig(cached_content=cache.name),\n)\nresponse = chat.send_message(\n    message=\"I didn't understand that last part, could you explain it in simpler language?\"\n)\nprint(\"\\n\\nmodel:  \", response.text)\n[cache.py](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/python/cache.py#L81-L120)\n\n```\n\n### Node.js\n```\n// Make sure to include the following import:\n// import {GoogleGenAI} from '@google/genai';\nconstai=newGoogleGenAI({apiKey:process.env.GEMINI_API_KEY});\nconstmodelName=\"gemini-1.5-flash-001\";\nconstsystemInstruction=\"You are an expert analyzing transcripts.\";\n\n// Create a chat session with the system instruction.\nconstchat=ai.chats.create({\nmodel:modelName,\nconfig:{systemInstruction:systemInstruction},\n});\nconstfilePath=path.join(media,\"a11.txt\");\nconstdocument=awaitai.files.upload({\nfile:filePath,\nconfig:{mimeType:\"text/plain\"},\n});\nconsole.log(\"Uploaded file name:\",document.name);\n\nletresponse=awaitchat.sendMessage({\nmessage:createUserContent([\n\"Hi, could you summarize this transcript?\",\ncreatePartFromUri(document.uri,document.mimeType),\n]),\n});\nconsole.log(\"\\n\\nmodel:\",response.text);\n\nresponse=awaitchat.sendMessage({\nmessage:\"Okay, could you tell me more about the trans-lunar injection\",\n});\nconsole.log(\"\\n\\nmodel:\",response.text);\n\n// To cache the conversation so far, pass the chat history as the list of contents.\nconstchatHistory=chat.getHistory();\nconstcache=awaitai.caches.create({\nmodel:modelName,\nconfig:{\ncontents:chatHistory,\nsystemInstruction:systemInstruction,\n},\n});\n\n// Continue the conversation using the cached content.\nconstchatWithCache=ai.chats.create({\nmodel:modelName,\nconfig:{cachedContent:cache.name},\n});\nresponse=awaitchatWithCache.sendMessage({\nmessage:\n\"I didn't understand that last part, could you explain it in simpler language?\",\n});\nconsole.log(\"\\n\\nmodel:\",response.text);\n[cache.js](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/javascript/cache.js#L111-L161)\n\n```\n\n### Go\n```\nctx:=context.Background()\nclient,err:=genai.NewClient(ctx,&genai.ClientConfig{\nAPIKey:os.Getenv(\"GEMINI_API_KEY\"),\nBackend:genai.BackendGeminiAPI,\n})\niferr!=nil{\nlog.Fatal(err)\n}\n\nmodelName:=\"gemini-1.5-flash-001\"\nsystemInstruction:=\"You are an expert analyzing transcripts.\"\n\n// Create initial chat with a system instruction.\nchat,err:=client.Chats.Create(ctx,modelName,&genai.GenerateContentConfig{\nSystemInstruction:genai.NewContentFromText(systemInstruction,genai.RoleUser),\n},nil)\niferr!=nil{\nlog.Fatal(err)\n}\n\ndocument,err:=client.Files.UploadFromPath(\nctx,\nfilepath.Join(getMedia(),\"a11.txt\"),\n&genai.UploadFileConfig{\nMIMEType:\"text/plain\",\n},\n)\niferr!=nil{\nlog.Fatal(err)\n}\n\n// Send first message with the transcript.\nparts:=make([]genai.Part,2)\nparts[0]=genai.Part{Text:\"Hi, could you summarize this transcript?\"}\nparts[1]=genai.Part{\nFileData:&genai.FileData{\nFileURI:document.URI,\nMIMEType:document.MIMEType,\n},\n}\n\n// Send chat message.\nresp,err:=chat.SendMessage(ctx,parts...)\niferr!=nil{\nlog.Fatal(err)\n}\nfmt.Println(\"\\n\\nmodel: \",resp.Text())\n\nresp,err=chat.SendMessage(\nctx,\ngenai.Part{\nText:\"Okay, could you tell me more about the trans-lunar injection\",\n},\n)\niferr!=nil{\nlog.Fatal(err)\n}\nfmt.Println(\"\\n\\nmodel: \",resp.Text())\n\n// To cache the conversation so far, pass the chat history as the list of contents.\ncache,err:=client.Caches.Create(ctx,modelName,&genai.CreateCachedContentConfig{\nContents:chat.History(false),\nSystemInstruction:genai.NewContentFromText(systemInstruction,genai.RoleUser),\n})\niferr!=nil{\nlog.Fatal(err)\n}\n\n// Continue the conversation using the cached history.\nchat,err=client.Chats.Create(ctx,modelName,&genai.GenerateContentConfig{\nCachedContent:cache.Name,\n},nil)\niferr!=nil{\nlog.Fatal(err)\n}\n\nresp,err=chat.SendMessage(\nctx,\ngenai.Part{\nText:\"I didn't understand that last part, could you explain it in simpler language?\",\n},\n)\niferr!=nil{\nlog.Fatal(err)\n}\nfmt.Println(\"\\n\\nmodel: \",resp.Text())\n[cache.go](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/go/cache.go#L140-L225)\n\n```\n\n### Response body\nIf successful, the response body contains a newly created instance of .\n## Method: cachedContents.list\n  * [Query parameters](https://ai.google.dev/api/caching#body.QUERY_PARAMETERS)\n  * [Response body](https://ai.google.dev/api/caching#body.response_body)\n    * [JSON representation](https://ai.google.dev/api/caching#body.ListCachedContentsResponse.SCHEMA_REPRESENTATION)\n\n\nLists CachedContents.\n### Endpoint\n`https://generativelanguage.googleapis.com/v1beta/cachedContents`\n### Query parameters\n`pageSize` `integer`\nOptional. The maximum number of cached contents to return. The service may return fewer than this value. If unspecified, some default (under maximum) number of items will be returned. The maximum value is 1000; values above 1000 will be coerced to 1000.\n`pageToken` `string`\nOptional. A page token, received from a previous `cachedContents.list` call. Provide this to retrieve the subsequent page.\nWhen paginating, all other parameters provided to `cachedContents.list` must match the call that provided the page token.\n### Request body\nThe request body must be empty.\n### Response body\nResponse with CachedContents list.\nIf successful, the response body contains data with the following structure:\nFields \n`cachedContents[]` `object ()`\nList of cached contents.\n`nextPageToken` `string`\nA token, which can be sent as `pageToken` to retrieve the next page. If this field is omitted, there are no subsequent pages.\nJSON representation  \n---  \n```\n{\n  \"cachedContents\": [\n    {\n      object ()\n    }\n  ],\n  \"nextPageToken\": string\n}\n```\n  \n## Method: cachedContents.get\n  * [Path parameters](https://ai.google.dev/api/caching#body.PATH_PARAMETERS)\n  * [Example request](https://ai.google.dev/api/caching#body.codeSnippets)\n\n\nReads CachedContent resource.\n### Endpoint\n`https://generativelanguage.googleapis.com/v1beta/{name=cachedContents/*}`\n### Path parameters\n`name` `string`\nRequired. The resource name referring to the content cache entry. Format: `cachedContents/{id}` It takes the form `cachedContents/{cachedcontent}`.\n### Request body\nThe request body must be empty.\n### Example request\n### Python\n```\nfromgoogleimport genai\n\nclient = genai.Client()\ndocument = client.files.upload(file=media / \"a11.txt\")\nmodel_name = \"gemini-1.5-flash-001\"\n\ncache = client.caches.create(\n    model=model_name,\n    config={\n        \"contents\": [document],\n        \"system_instruction\": \"You are an expert analyzing transcripts.\",\n    },\n)\nprint(client.caches.get(name=cache.name))\n[cache.py](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/python/cache.py#L144-L157)\n\n```\n\n### Node.js\n```\n// Make sure to include the following import:\n// import {GoogleGenAI} from '@google/genai';\nconstai=newGoogleGenAI({apiKey:process.env.GEMINI_API_KEY});\nconstfilePath=path.join(media,\"a11.txt\");\nconstdocument=awaitai.files.upload({\nfile:filePath,\nconfig:{mimeType:\"text/plain\"},\n});\nconsole.log(\"Uploaded file name:\",document.name);\nconstmodelName=\"gemini-1.5-flash-001\";\n\nconstcontents=[\ncreateUserContent(createPartFromUri(document.uri,document.mimeType)),\n];\n\nconstcache=awaitai.caches.create({\nmodel:modelName,\nconfig:{\ncontents:contents,\nsystemInstruction:\"You are an expert analyzing transcripts.\",\n},\n});\nconstretrievedCache=awaitai.caches.get({name:cache.name});\nconsole.log(\"Retrieved Cache:\",retrievedCache);\n[cache.js](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/javascript/cache.js#L199-L222)\n\n```\n\n### Go\n```\nctx:=context.Background()\nclient,err:=genai.NewClient(ctx,&genai.ClientConfig{\nAPIKey:os.Getenv(\"GEMINI_API_KEY\"),\nBackend:genai.BackendGeminiAPI,\n})\niferr!=nil{\nlog.Fatal(err)\n}\n\nmodelName:=\"gemini-1.5-flash-001\"\ndocument,err:=client.Files.UploadFromPath(\nctx,\nfilepath.Join(getMedia(),\"a11.txt\"),\n&genai.UploadFileConfig{\nMIMEType:\"text/plain\",\n},\n)\niferr!=nil{\nlog.Fatal(err)\n}\nparts:=[]*genai.Part{\ngenai.NewPartFromURI(document.URI,document.MIMEType),\n}\ncontents:=[]*genai.Content{\ngenai.NewContentFromParts(parts,genai.RoleUser),\n}\n\ncache,err:=client.Caches.Create(ctx,modelName,&genai.CreateCachedContentConfig{\nContents:contents,\nSystemInstruction:genai.NewContentFromText(\n\"You are an expert analyzing transcripts.\",genai.RoleUser,\n),\n})\niferr!=nil{\nlog.Fatal(err)\n}\n\ncache,err=client.Caches.Get(ctx,cache.Name,&genai.GetCachedContentConfig{})\niferr!=nil{\nlog.Fatal(err)\n}\nfmt.Println(\"Retrieved cache:\")\nfmt.Println(cache)\n[cache.go](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/go/cache.go#L285-L327)\n\n```\n\n### Shell\n```\ncurl\"https://generativelanguage.googleapis.com/v1beta/$CACHE_NAME?key=$GEMINI_API_KEY\"\n```\n\n### Response body\nIf successful, the response body contains an instance of .\n## Method: cachedContents.patch\n  * [Path parameters](https://ai.google.dev/api/caching#body.PATH_PARAMETERS)\n  * [Query parameters](https://ai.google.dev/api/caching#body.QUERY_PARAMETERS)\n  * [Example request](https://ai.google.dev/api/caching#body.codeSnippets)\n\n\nUpdates CachedContent resource (only expiration is updatable).\n### Endpoint\npatch  `https://generativelanguage.googleapis.com/v1beta/{cachedContent.name=cachedContents/*}`\n`PATCH https://generativelanguage.googleapis.com/v1beta/{cachedContent.name=cachedContents/*}`\n### Path parameters\n`cachedContent.name` `string`\nOutput only. Identifier. The resource name referring to the cached content. Format: `cachedContents/{id}` It takes the form `cachedContents/{cachedcontent}`.\n### Query parameters\n`updateMask` `string ( format)`\nThe list of fields to update.\nThis is a comma-separated list of fully qualified names of fields. Example: `\"user.displayName,photo\"`.\n### Request body\nThe request body contains an instance of .\nFields \n`expiration` `Union type`\nSpecifies when this resource will expire. `expiration` can be only one of the following:\n`expireTime` `string ( format)`\nTimestamp in UTC of when this resource is considered expired. This is _always_ provided on output, regardless of what was sent on input.\nUses RFC 3339, where generated output will always be Z-normalized and use 0, 3, 6 or 9 fractional digits. Offsets other than \"Z\" are also accepted. Examples: `\"2014-10-02T15:01:23Z\"`, `\"2014-10-02T15:01:23.045123456Z\"` or `\"2014-10-02T15:01:23+05:30\"`.\n`string ( format)`\nInput only. New TTL for this resource, input only.\nA duration in seconds with up to nine fractional digits, ending with '`s`'. Example: `\"3.5s\"`.\n### Example request\n### Python\n```\nfromgoogleimport genai\nfromgoogle.genaiimport types\nimportdatetime\n\nclient = genai.Client()\ndocument = client.files.upload(file=media / \"a11.txt\")\nmodel_name = \"gemini-1.5-flash-001\"\n\ncache = client.caches.create(\n    model=model_name,\n    config={\n        \"contents\": [document],\n        \"system_instruction\": \"You are an expert analyzing transcripts.\",\n    },\n)\n\n# Update the cache's time-to-live (ttl)\nttl = f\"{int(datetime.timedelta(hours=2).total_seconds())}s\"\nclient.caches.update(\n    name=cache.name, config=types.UpdateCachedContentConfig(ttl=ttl)\n)\nprint(f\"After update:\\n{cache}\")\n\n# Alternatively, update the expire_time directly\n# Update the expire_time directly in valid RFC 3339 format (UTC with a \"Z\" suffix)\nexpire_time = (\n    (\n        datetime.datetime.now(datetime.timezone.utc)\n        + datetime.timedelta(minutes=15)\n    )\n    .isoformat()\n    .replace(\"+00:00\", \"Z\")\n)\nclient.caches.update(\n    name=cache.name,\n    config=types.UpdateCachedContentConfig(expire_time=expire_time),\n)\n[cache.py](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/python/cache.py#L184-L220)\n\n```\n\n### Node.js\n```\n// Make sure to include the following import:\n// import {GoogleGenAI} from '@google/genai';\nconstai=newGoogleGenAI({apiKey:process.env.GEMINI_API_KEY});\nconstfilePath=path.join(media,\"a11.txt\");\nconstdocument=awaitai.files.upload({\nfile:filePath,\nconfig:{mimeType:\"text/plain\"},\n});\nconsole.log(\"Uploaded file name:\",document.name);\nconstmodelName=\"gemini-1.5-flash-001\";\n\nconstcontents=[\ncreateUserContent(createPartFromUri(document.uri,document.mimeType)),\n];\n\nletcache=awaitai.caches.create({\nmodel:modelName,\nconfig:{\ncontents:contents,\nsystemInstruction:\"You are an expert analyzing transcripts.\",\n},\n});\n\n// Update the cache's time-to-live (ttl)\nconstttl=`${2*3600}s`;// 2 hours in seconds\ncache=awaitai.caches.update({\nname:cache.name,\nconfig:{ttl},\n});\nconsole.log(\"After update (TTL):\",cache);\n\n// Alternatively, update the expire_time directly (in RFC 3339 format with a \"Z\" suffix)\nconstexpireTime=newDate(Date.now()+15*60000)\n.toISOString()\n.replace(/\\.\\d{3}Z$/,\"Z\");\ncache=awaitai.caches.update({\nname:cache.name,\nconfig:{expireTime:expireTime},\n});\nconsole.log(\"After update (expire_time):\",cache);\n[cache.js](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/javascript/cache.js#L272-L311)\n\n```\n\n### Go\n```\nctx:=context.Background()\nclient,err:=genai.NewClient(ctx,&genai.ClientConfig{\nAPIKey:os.Getenv(\"GEMINI_API_KEY\"),\nBackend:genai.BackendGeminiAPI,\n})\niferr!=nil{\nlog.Fatal(err)\n}\n\nmodelName:=\"gemini-1.5-flash-001\"\ndocument,err:=client.Files.UploadFromPath(\nctx,\nfilepath.Join(getMedia(),\"a11.txt\"),\n&genai.UploadFileConfig{\nMIMEType:\"text/plain\",\n},\n)\niferr!=nil{\nlog.Fatal(err)\n}\nparts:=[]*genai.Part{\ngenai.NewPartFromURI(document.URI,document.MIMEType),\n}\ncontents:=[]*genai.Content{\ngenai.NewContentFromParts(parts,genai.RoleUser),\n}\n\ncache,err:=client.Caches.Create(ctx,modelName,&genai.CreateCachedContentConfig{\nContents:contents,\nSystemInstruction:genai.NewContentFromText(\n\"You are an expert analyzing transcripts.\",genai.RoleUser,\n),\n})\niferr!=nil{\nlog.Fatal(err)\n}\n\n_,err=client.Caches.Delete(ctx,cache.Name,&genai.DeleteCachedContentConfig{})\niferr!=nil{\nlog.Fatal(err)\n}\nfmt.Println(\"Cache deleted:\",cache.Name)\n[cache.go](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/go/cache.go#L237-L278)\n\n```\n\n### Shell\n```\ncurl-XPATCH\"https://generativelanguage.googleapis.com/v1beta/$CACHE_NAME?key=$GEMINI_API_KEY\"\\\n-H'Content-Type: application/json'\\\n-d'{\"ttl\": \"600s\"}'\n```\n\n### Response body\nIf successful, the response body contains an instance of .\n## Method: cachedContents.delete\n  * [Path parameters](https://ai.google.dev/api/caching#body.PATH_PARAMETERS)\n  * [Example request](https://ai.google.dev/api/caching#body.codeSnippets)\n\n\nDeletes CachedContent resource.\n### Endpoint\ndelete  `https://generativelanguage.googleapis.com/v1beta/{name=cachedContents/*}`\n### Path parameters\n`name` `string`\nRequired. The resource name referring to the content cache entry Format: `cachedContents/{id}` It takes the form `cachedContents/{cachedcontent}`.\n### Request body\nThe request body must be empty.\n### Example request\n### Python\n```\nfromgoogleimport genai\n\nclient = genai.Client()\ndocument = client.files.upload(file=media / \"a11.txt\")\nmodel_name = \"gemini-1.5-flash-001\"\n\ncache = client.caches.create(\n    model=model_name,\n    config={\n        \"contents\": [document],\n        \"system_instruction\": \"You are an expert analyzing transcripts.\",\n    },\n)\nclient.caches.delete(name=cache.name)\n[cache.py](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/python/cache.py#L126-L139)\n\n```\n\n### Node.js\n```\n// Make sure to include the following import:\n// import {GoogleGenAI} from '@google/genai';\nconstai=newGoogleGenAI({apiKey:process.env.GEMINI_API_KEY});\nconstfilePath=path.join(media,\"a11.txt\");\nconstdocument=awaitai.files.upload({\nfile:filePath,\nconfig:{mimeType:\"text/plain\"},\n});\nconsole.log(\"Uploaded file name:\",document.name);\nconstmodelName=\"gemini-1.5-flash-001\";\n\nconstcontents=[\ncreateUserContent(createPartFromUri(document.uri,document.mimeType)),\n];\n\nconstcache=awaitai.caches.create({\nmodel:modelName,\nconfig:{\ncontents:contents,\nsystemInstruction:\"You are an expert analyzing transcripts.\",\n},\n});\nawaitai.caches.delete({name:cache.name});\nconsole.log(\"Cache deleted:\",cache.name);\n[cache.js](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/javascript/cache.js#L170-L193)\n\n```\n\n### Go\n```\nctx:=context.Background()\nclient,err:=genai.NewClient(ctx,&genai.ClientConfig{\nAPIKey:os.Getenv(\"GEMINI_API_KEY\"),\nBackend:genai.BackendGeminiAPI,\n})\niferr!=nil{\nlog.Fatal(err)\n}\n\nmodelName:=\"gemini-1.5-flash-001\"\ndocument,err:=client.Files.UploadFromPath(\nctx,\nfilepath.Join(getMedia(),\"a11.txt\"),\n&genai.UploadFileConfig{\nMIMEType:\"text/plain\",\n},\n)\niferr!=nil{\nlog.Fatal(err)\n}\nparts:=[]*genai.Part{\ngenai.NewPartFromURI(document.URI,document.MIMEType),\n}\ncontents:=[]*genai.Content{\ngenai.NewContentFromParts(parts,genai.RoleUser),\n}\n\ncache,err:=client.Caches.Create(ctx,modelName,&genai.CreateCachedContentConfig{\nContents:contents,\nSystemInstruction:genai.NewContentFromText(\n\"You are an expert analyzing transcripts.\",genai.RoleUser,\n),\n})\niferr!=nil{\nlog.Fatal(err)\n}\n\n_,err=client.Caches.Delete(ctx,cache.Name,&genai.DeleteCachedContentConfig{})\niferr!=nil{\nlog.Fatal(err)\n}\nfmt.Println(\"Cache deleted:\",cache.Name)\n[cache.go](https://github.com/google-gemini/api-examples/blob/856e8a0f566a2810625cecabba6e2ab1fe97e496/go/cache.go#L237-L278)\n\n```\n\n### Shell\n```\ncurl-XDELETE\"https://generativelanguage.googleapis.com/v1beta/$CACHE_NAME?key=$GEMINI_API_KEY\"\n```\n\n### Response body\nIf successful, the response body is an empty JSON object.\n## REST Resource: cachedContents\n  * [Resource: CachedContent](https://ai.google.dev/api/caching#CachedContent)\n    * [JSON representation](https://ai.google.dev/api/caching#CachedContent.SCHEMA_REPRESENTATION)\n  * [Content](https://ai.google.dev/api/caching#Content)\n    * [JSON representation](https://ai.google.dev/api/caching#Content.SCHEMA_REPRESENTATION)\n  * [Part](https://ai.google.dev/api/caching#Part)\n    * [JSON representation](https://ai.google.dev/api/caching#Part.SCHEMA_REPRESENTATION)\n  * [Blob](https://ai.google.dev/api/caching#Blob)\n    * [JSON representation](https://ai.google.dev/api/caching#Blob.SCHEMA_REPRESENTATION)\n  * [FunctionCall](https://ai.google.dev/api/caching#FunctionCall)\n    * [JSON representation](https://ai.google.dev/api/caching#FunctionCall.SCHEMA_REPRESENTATION)\n  * [FunctionResponse](https://ai.google.dev/api/caching#FunctionResponse)\n    * [JSON representation](https://ai.google.dev/api/caching#FunctionResponse.SCHEMA_REPRESENTATION)\n  * [FunctionResponsePart](https://ai.google.dev/api/caching#FunctionResponsePart)\n    * [JSON representation](https://ai.google.dev/api/caching#FunctionResponsePart.SCHEMA_REPRESENTATION)\n  * [FunctionResponseBlob](https://ai.google.dev/api/caching#FunctionResponseBlob)\n    * [JSON representation](https://ai.google.dev/api/caching#FunctionResponseBlob.SCHEMA_REPRESENTATION)\n  * [FileData](https://ai.google.dev/api/caching#FileData)\n    * [JSON representation](https://ai.google.dev/api/caching#FileData.SCHEMA_REPRESENTATION)\n  * [ExecutableCode](https://ai.google.dev/api/caching#ExecutableCode)\n    * [JSON representation](https://ai.google.dev/api/caching#ExecutableCode.SCHEMA_REPRESENTATION)\n  * [CodeExecutionResult](https://ai.google.dev/api/caching#CodeExecutionResult)\n    * [JSON representation](https://ai.google.dev/api/caching#CodeExecutionResult.SCHEMA_REPRESENTATION)\n  * [VideoMetadata](https://ai.google.dev/api/caching#VideoMetadata)\n    * [JSON representation](https://ai.google.dev/api/caching#VideoMetadata.SCHEMA_REPRESENTATION)\n  * [Tool](https://ai.google.dev/api/caching#Tool)\n    * [JSON representation](https://ai.google.dev/api/caching#Tool.SCHEMA_REPRESENTATION)\n  * [FunctionDeclaration](https://ai.google.dev/api/caching#FunctionDeclaration)\n    * [JSON representation](https://ai.google.dev/api/caching#FunctionDeclaration.SCHEMA_REPRESENTATION)\n  * [Schema](https://ai.google.dev/api/caching#Schema)\n    * [JSON representation](https://ai.google.dev/api/caching#Schema.SCHEMA_REPRESENTATION)\n  * [GoogleSearchRetrieval](https://ai.google.dev/api/caching#GoogleSearchRetrieval)\n    * [JSON representation](https://ai.google.dev/api/caching#GoogleSearchRetrieval.SCHEMA_REPRESENTATION)\n  * [DynamicRetrievalConfig](https://ai.google.dev/api/caching#DynamicRetrievalConfig)\n    * [JSON representation](https://ai.google.dev/api/caching#DynamicRetrievalConfig.SCHEMA_REPRESENTATION)\n  * [CodeExecution](https://ai.google.dev/api/caching#CodeExecution)\n  * [GoogleSearch](https://ai.google.dev/api/caching#GoogleSearch)\n    * [JSON representation](https://ai.google.dev/api/caching#GoogleSearch.SCHEMA_REPRESENTATION)\n  * [Interval](https://ai.google.dev/api/caching#Interval)\n    * [JSON representation](https://ai.google.dev/api/caching#Interval.SCHEMA_REPRESENTATION)\n  * [ComputerUse](https://ai.google.dev/api/caching#ComputerUse)\n    * [JSON representation](https://ai.google.dev/api/caching#ComputerUse.SCHEMA_REPRESENTATION)\n  * [FileSearch](https://ai.google.dev/api/caching#FileSearch)\n    * [JSON representation](https://ai.google.dev/api/caching#FileSearch.SCHEMA_REPRESENTATION)\n  * [GoogleMaps](https://ai.google.dev/api/caching#GoogleMaps)\n    * [JSON representation](https://ai.google.dev/api/caching#GoogleMaps.SCHEMA_REPRESENTATION)\n  * [ToolConfig](https://ai.google.dev/api/caching#ToolConfig)\n    * [JSON representation](https://ai.google.dev/api/caching#ToolConfig.SCHEMA_REPRESENTATION)\n  * [FunctionCallingConfig](https://ai.google.dev/api/caching#FunctionCallingConfig)\n    * [JSON representation](https://ai.google.dev/api/caching#FunctionCallingConfig.SCHEMA_REPRESENTATION)\n  * [RetrievalConfig](https://ai.google.dev/api/caching#RetrievalConfig)\n    * [JSON representation](https://ai.google.dev/api/caching#RetrievalConfig.SCHEMA_REPRESENTATION)\n  * [LatLng](https://ai.google.dev/api/caching#LatLng)\n    * [JSON representation](https://ai.google.dev/api/caching#LatLng.SCHEMA_REPRESENTATION)\n  * [UsageMetadata](https://ai.google.dev/api/caching#UsageMetadata)\n    * [JSON representation](https://ai.google.dev/api/caching#UsageMetadata.SCHEMA_REPRESENTATION)\n\n\n## Resource: CachedContent\nContent that has been preprocessed and can be used in subsequent request to GenerativeService.\nCached content can be only used with model it was created for.\nFields \n`contents[]` `object ()`\nOptional. Input only. Immutable. The content to cache.\n`tools[]` `object ()`\nOptional. Input only. Immutable. A list of `Tools` the model may use to generate the next response\n`createTime` `string ( format)`\nOutput only. Creation time of the cache entry.\nUses RFC 3339, where generated output will always be Z-normalized and use 0, 3, 6 or 9 fractional digits. Offsets other than \"Z\" are also accepted. Examples: `\"2014-10-02T15:01:23Z\"`, `\"2014-10-02T15:01:23.045123456Z\"` or `\"2014-10-02T15:01:23+05:30\"`.\n`updateTime` `string ( format)`\nOutput only. When the cache entry was last updated in UTC time.\nUses RFC 3339, where generated output will always be Z-normalized and use 0, 3, 6 or 9 fractional digits. Offsets other than \"Z\" are also accepted. Examples: `\"2014-10-02T15:01:23Z\"`, `\"2014-10-02T15:01:23.045123456Z\"` or `\"2014-10-02T15:01:23+05:30\"`.\n`usageMetadata` `object ()`\nOutput only. Metadata on the usage of the cached content.\n`expiration` `Union type`\nSpecifies when this resource will expire. `expiration` can be only one of the following:\n`expireTime` `string ( format)`\nTimestamp in UTC of when this resource is considered expired. This is _always_ provided on output, regardless of what was sent on input.\nUses RFC 3339, where generated output will always be Z-normalized and use 0, 3, 6 or 9 fractional digits. Offsets other than \"Z\" are also accepted. Examples: `\"2014-10-02T15:01:23Z\"`, `\"2014-10-02T15:01:23.045123456Z\"` or `\"2014-10-02T15:01:23+05:30\"`.\n`string ( format)`\nInput only. New TTL for this resource, input only.\nA duration in seconds with up to nine fractional digits, ending with '`s`'. Example: `\"3.5s\"`.\n`name` `string`\nOutput only. Identifier. The resource name referring to the cached content. Format: `cachedContents/{id}`\n`displayName` `string`\nOptional. Immutable. The user-generated meaningful display name of the cached content. Maximum 128 Unicode characters.\n`model` `string`\nRequired. Immutable. The name of the `Model` to use for cached content Format: `models/{model}`\n`systemInstruction` `object ()`\nOptional. Input only. Immutable. Developer set system instruction. Currently text only.\n`toolConfig` `object ()`\nOptional. Input only. Immutable. Tool config. This config is shared for all tools.\nJSON representation  \n---  \n```\n{\n  \"contents\": [\n    {\n      object ()\n    }\n  ],\n  \"tools\": [\n    {\n      object ()\n    }\n  ],\n  \"createTime\": string,\n  \"updateTime\": string,\n  \"usageMetadata\": {\n    object ()\n  },\n\n  // expiration\n  \"expireTime\": string,\n  \"ttl\": string\n  // Union type\n  \"name\": string,\n  \"displayName\": string,\n  \"model\": string,\n  \"systemInstruction\": {\n    object ()\n  },\n  \"toolConfig\": {\n    object ()\n  }\n}\n```\n  \n## Content\nThe base structured datatype containing multi-part content of a message.\nA `Content` includes a `role` field designating the producer of the `Content` and a `parts` field containing multi-part data that contains the content of the message turn.\nFields \n`parts[]` `object ()`\nOrdered `Parts` that constitute a single message. Parts may have different MIME types.\n`role` `string`\nOptional. The producer of the content. Must be either 'user' or 'model'.\nUseful to set for multi-turn conversations, otherwise can be left blank or unset.\nJSON representation  \n---  \n```\n{\n  \"parts\": [\n    {\n      object ()\n    }\n  ],\n  \"role\": string\n}\n```\n  \n## Part\nA datatype containing media that is part of a multi-part `Content` message.\nA `Part` consists of data which has an associated datatype. A `Part` can only contain one of the accepted types in `Part.data`.\nA `Part` must have a fixed IANA MIME type identifying the type and subtype of the media if the `inlineData` field is filled with raw bytes.\nFields \n`thought` `boolean`\nOptional. Indicates if the part is thought from the model.\n`thoughtSignature` `string (bytes[](https://developers.google.com/discovery/v1/type-format) format)`\nOptional. An opaque signature for the thought so it can be reused in subsequent requests.\nA base64-encoded string.\n`partMetadata` `object ( format)`\nCustom metadata associated with the Part. Agents using genai.Part as content representation may need to keep track of the additional information. For example it can be name of a file/source from which the Part originates or a way to multiplex multiple Part streams.\n`data` `Union type`\n`data` can be only one of the following:\n`text` `string`\nInline text.\n`inlineData` `object ()`\nInline media bytes.\n`functionCall` `object ()`\nA predicted `FunctionCall` returned from the model that contains a string representing the `FunctionDeclaration.name` with the arguments and their values.\n`functionResponse` `object (`FunctionResponse[](https://ai.google.dev/api/caching#FunctionResponse)`)`\nThe result output of a `FunctionCall` that contains a string representing the `FunctionDeclaration.name` and a structured JSON object containing any output from the function is used as context to the model.\n`fileData` `object ()`\nURI based data.\n`executableCode` `object ()`\nCode generated by the model that is meant to be executed.\n`codeExecutionResult` `object (`CodeExecutionResult[](https://ai.google.dev/api/caching#CodeExecutionResult)`)`\nResult of executing the `ExecutableCode`.\n`metadata` `Union type`\nControls extra preprocessing of data. `metadata` can be only one of the following:\n`videoMetadata` `object ()`\nOptional. Video metadata. The metadata should only be specified while the video data is presented in inlineData or fileData.\nJSON representation  \n---  \n```\n{\n  \"thought\": boolean,\n  \"thoughtSignature\": string,\n  \"partMetadata\": {\n    object\n  },\n\n  // data\n  \"text\": string,\n  \"inlineData\": {\n    object ()\n  },\n  \"functionCall\": {\n    object ()\n  },\n  \"functionResponse\": {\n    object (FunctionResponse[](https://ai.google.dev/api/caching#FunctionResponse))\n  },\n  \"fileData\": {\n    object ()\n  },\n  \"executableCode\": {\n    object ()\n  },\n  \"codeExecutionResult\": {\n    object (CodeExecutionResult[](https://ai.google.dev/api/caching#CodeExecutionResult))\n  }\n  // Union type\n\n  // metadata\n  \"videoMetadata\": {\n    object ()\n  }\n  // Union type\n}\n```\n  \n## Blob\nRaw media bytes.\nText should not be sent as raw bytes, use the 'text' field.\nFields \n`mimeType` `string`\nThe IANA standard MIME type of the source data. Examples: - image/png - image/jpeg If an unsupported MIME type is provided, an error will be returned. For a complete list of supported types, see [Supported file formats](https://ai.google.dev/gemini-api/docs/prompting_with_media#supported_file_formats).\n`data` `string (bytes[](https://developers.google.com/discovery/v1/type-format) format)`\nRaw bytes for media formats.\nA base64-encoded string.\nJSON representation  \n---  \n```\n{\n  \"mimeType\": string,\n  \"data\": string\n}\n```\n  \n## FunctionCall\nA predicted `FunctionCall` returned from the model that contains a string representing the `FunctionDeclaration.name` with the arguments and their values.\nFields \n`string`\nOptional. The unique id of the function call. If populated, the client to execute the `functionCall` and return the response with the matching `id`.\n`name` `string`\nRequired. The name of the function to call. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.\n`args` `object ( format)`\nOptional. The function parameters and values in JSON object format.\nJSON representation  \n---  \n```\n{\n  \"id\": string,\n  \"name\": string,\n  \"args\": {\n    object\n  }\n}\n```\n  \n## FunctionResponse\nThe result output from a `FunctionCall` that contains a string representing the `FunctionDeclaration.name` and a structured JSON object containing any output from the function is used as context to the model. This should contain the result of a`FunctionCall` made based on model prediction.\nFields \n`string`\nOptional. The id of the function call this response is for. Populated by the client to match the corresponding function call `id`.\n`name` `string`\nRequired. The name of the function to call. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.\n`response` `object ( format)`\nRequired. The function response in JSON object format. Callers can use any keys of their choice that fit the function's syntax to return the function output, e.g. \"output\", \"result\", etc. In particular, if the function call failed to execute, the response can have an \"error\" key to return error details to the model.\n`parts[]` `object (`FunctionResponsePart[](https://ai.google.dev/api/caching#FunctionResponsePart)`)`\nOptional. Ordered `Parts` that constitute a function response. Parts may have different IANA MIME types.\n`willContinue` `boolean`\nOptional. Signals that function call continues, and more responses will be returned, turning the function call into a generator. Is only applicable to NON_BLOCKING function calls, is ignored otherwise. If set to false, future responses will not be considered. It is allowed to return empty `response` with `willContinue=False` to signal that the function call is finished. This may still trigger the model generation. To avoid triggering the generation and finish the function call, additionally set `scheduling` to `SILENT`.\n`scheduling` `enum ()`\nOptional. Specifies how the response should be scheduled in the conversation. Only applicable to NON_BLOCKING function calls, is ignored otherwise. Defaults to WHEN_IDLE.\nJSON representation  \n---  \n```\n{\n  \"id\": string,\n  \"name\": string,\n  \"response\": {\n    object\n  },\n  \"parts\": [\n    {\n      object (FunctionResponsePart[](https://ai.google.dev/api/caching#FunctionResponsePart))\n    }\n  ],\n  \"willContinue\": boolean,\n  \"scheduling\": enum ()\n}\n```\n  \n## FunctionResponsePart\nA datatype containing media that is part of a `FunctionResponse` message.\nA `FunctionResponsePart` consists of data which has an associated datatype. A `FunctionResponsePart` can only contain one of the accepted types in `FunctionResponsePart.data`.\nA `FunctionResponsePart` must have a fixed IANA MIME type identifying the type and subtype of the media if the `inlineData` field is filled with raw bytes.\nFields \n`data` `Union type`\nThe data of the function response part. `data` can be only one of the following:\n`inlineData` `object (`FunctionResponseBlob[](https://ai.google.dev/api/caching#FunctionResponseBlob)`)`\nInline media bytes.\nJSON representation  \n---  \n```\n{\n\n  // data\n  \"inlineData\": {\n    object (FunctionResponseBlob[](https://ai.google.dev/api/caching#FunctionResponseBlob))\n  }\n  // Union type\n}\n```\n  \n## FunctionResponseBlob\nRaw media bytes for function response.\nText should not be sent as raw bytes, use the 'FunctionResponse.response' field.\nFields \n`mimeType` `string`\nThe IANA standard MIME type of the source data. Examples: - image/png - image/jpeg If an unsupported MIME type is provided, an error will be returned. For a complete list of supported types, see [Supported file formats](https://ai.google.dev/gemini-api/docs/prompting_with_media#supported_file_formats).\n`data` `string (bytes[](https://developers.google.com/discovery/v1/type-format) format)`\nRaw bytes for media formats.\nA base64-encoded string.\nJSON representation  \n---  \n```\n{\n  \"mimeType\": string,\n  \"data\": string\n}\n```\n  \n## Scheduling\nSpecifies how the response should be scheduled in the conversation.\nEnums  \n---  \n`SCHEDULING_UNSPECIFIED` | This value is unused.  \n`SILENT` | Only add the result to the conversation context, do not interrupt or trigger generation.  \n`WHEN_IDLE` | Add the result to the conversation context, and prompt to generate output without interrupting ongoing generation.  \n`INTERRUPT` | Add the result to the conversation context, interrupt ongoing generation and prompt to generate output.  \n## FileData\nURI based data.\nFields \n`mimeType` `string`\nOptional. The IANA standard MIME type of the source data.\n`fileUri` `string`\nRequired. URI.\nJSON representation  \n---  \n```\n{\n  \"mimeType\": string,\n  \"fileUri\": string\n}\n```\n  \n## ExecutableCode\nCode generated by the model that is meant to be executed, and the result returned to the model.\nOnly generated when using the `CodeExecution` tool, in which the code will be automatically executed, and a corresponding `CodeExecutionResult` will also be generated.\nFields \n`language` `enum ()`\nRequired. Programming language of the `code`.\n`code` `string`\nRequired. The code to be executed.\nJSON representation  \n---  \n```\n{\n  \"language\": enum (),\n  \"code\": string\n}\n```\n  \n## Language\nSupported programming languages for the generated code.\nEnums  \n---  \n`LANGUAGE_UNSPECIFIED` | Unspecified language. This value should not be used.  \n`PYTHON` | Python >= 3.10, with numpy and simpy available. Python is the default language.  \n## CodeExecutionResult\nResult of executing the `ExecutableCode`.\nOnly generated when using the `CodeExecution`, and always follows a `part` containing the `ExecutableCode`.\nFields \n`outcome` `enum ()`\nRequired. Outcome of the code execution.\n`output` `string`\nOptional. Contains stdout when code execution is successful, stderr or other description otherwise.\nJSON representation  \n---  \n```\n{\n  \"outcome\": enum (),\n  \"output\": string\n}\n```\n  \n## Outcome\nEnumeration of possible outcomes of the code execution.\nEnums  \n---  \n`OUTCOME_UNSPECIFIED` | Unspecified status. This value should not be used.  \n`OUTCOME_OK` | Code execution completed successfully.  \n`OUTCOME_FAILED` | Code execution finished but with a failure. `stderr` should contain the reason.  \n`OUTCOME_DEADLINE_EXCEEDED` | Code execution ran for too long, and was cancelled. There may or may not be a partial output present.  \n## VideoMetadata\nMetadata describes the input video content.\nFields \n`startOffset` `string ( format)`\nOptional. The start offset of the video.\nA duration in seconds with up to nine fractional digits, ending with '`s`'. Example: `\"3.5s\"`.\n`endOffset` `string ( format)`\nOptional. The end offset of the video.\nA duration in seconds with up to nine fractional digits, ending with '`s`'. Example: `\"3.5s\"`.\n`number`\nOptional. The frame rate of the video sent to the model. If not specified, the default value will be 1.0. The fps range is (0.0, 24.0].\nJSON representation  \n---  \n```\n{\n  \"startOffset\": string,\n  \"endOffset\": string,\n  \"fps\": number\n}\n```\n  \n## Tool\nTool details that the model may use to generate response.\nA `Tool` is a piece of code that enables the system to interact with external systems to perform an action, or set of actions, outside of knowledge and scope of the model.\nNext ID: 13\nFields \n`functionDeclarations[]` `object (`FunctionDeclaration[](https://ai.google.dev/api/caching#FunctionDeclaration)`)`\nOptional. A list of `FunctionDeclarations` available to the model that can be used for function calling.\nThe model or system does not execute the function. Instead the defined function may be returned as a with arguments to the client side for execution. The model may decide to call a subset of these functions by populating in the response. The next conversation turn may contain a with the \"function\" generation context for the next model turn.\n`googleSearchRetrieval` `object (`GoogleSearchRetrieval[](https://ai.google.dev/api/caching#GoogleSearchRetrieval)`)`\nOptional. Retrieval tool that is powered by Google search.\n`codeExecution` `object ()`\nOptional. Enables the model to execute code as part of generation.\n`googleSearch` `object ()`\nOptional. GoogleSearch tool type. Tool to support Google Search in Model. Powered by Google.\n`computerUse` `object ()`\nOptional. Tool to support the model interacting directly with the computer. If enabled, it automatically populates computer-use specific Function Declarations.\n`urlContext` `object ()`\nOptional. Tool to support URL context retrieval.\n`fileSearch` `object ()`\nOptional. FileSearch tool type. Tool to retrieve knowledge from Semantic Retrieval corpora.\n`googleMaps` `object ()`\nOptional. Tool that allows grounding the model's response with geospatial context related to the user's query.\nJSON representation  \n---  \n```\n{\n  \"functionDeclarations\": [\n    {\n      object (FunctionDeclaration[](https://ai.google.dev/api/caching#FunctionDeclaration))\n    }\n  ],\n  \"googleSearchRetrieval\": {\n    object (GoogleSearchRetrieval[](https://ai.google.dev/api/caching#GoogleSearchRetrieval))\n  },\n  \"codeExecution\": {\n    object ()\n  },\n  \"googleSearch\": {\n    object ()\n  },\n  \"computerUse\": {\n    object ()\n  },\n  \"urlContext\": {\n    object ()\n  },\n  \"fileSearch\": {\n    object ()\n  },\n  \"googleMaps\": {\n    object ()\n  }\n}\n```\n  \n## FunctionDeclaration\nStructured representation of a function declaration as defined by the [OpenAPI 3.03 specification](https://spec.openapis.org/oas/v3.0.3). Included in this declaration are the function name and parameters. This FunctionDeclaration is a representation of a block of code that can be used as a `Tool` by the model and executed by the client.\nFields \n`name` `string`\nRequired. The name of the function. Must be a-z, A-Z, 0-9, or contain underscores, colons, dots, and dashes, with a maximum length of 64.\n`description` `string`\nRequired. A brief description of the function.\n`behavior` `enum ()`\nOptional. Specifies the function Behavior. Currently only supported by the BidiGenerateContent method.\n`parameters` `object ()`\nOptional. Describes the parameters to this function. Reflects the Open API 3.03 Parameter Object string Key: the name of the parameter. Parameter names are case sensitive. Schema Value: the Schema defining the type used for the parameter.\n`parametersJsonSchema` `value ( format)`\nOptional. Describes the parameters to the function in JSON Schema format. The schema must describe an object where the properties are the parameters to the function. For example:\n```\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"name\": { \"type\": \"string\" },\n    \"age\": { \"type\": \"integer\" }\n  },\n  \"additionalProperties\": false,\n  \"required\": [\"name\", \"age\"],\n  \"propertyOrdering\": [\"name\", \"age\"]\n}\n\n```\n\nThis field is mutually exclusive with `parameters`.\n`response` `object ()`\nOptional. Describes the output from this function in JSON Schema format. Reflects the Open API 3.03 Response Object. The Schema defines the type used for the response value of the function.\n`responseJsonSchema` `value ( format)`\nOptional. Describes the output from this function in JSON Schema format. The value specified by the schema is the response value of the function.\nThis field is mutually exclusive with `response`.\nJSON representation  \n---  \n```\n{\n  \"name\": string,\n  \"description\": string,\n  \"behavior\": enum (),\n  \"parameters\": {\n    object ()\n  },\n  \"parametersJsonSchema\": value,\n  \"response\": {\n    object ()\n  },\n  \"responseJsonSchema\": value\n}\n```\n  \n## Schema\nThe `Schema` object allows the definition of input and output data types. These types can be objects, but also primitives and arrays. Represents a select subset of an [OpenAPI 3.0 schema object](https://spec.openapis.org/oas/v3.0.3#schema).\nFields \n`type` `enum ()`\nRequired. Data type.\n`format` `string`\nOptional. The format of the data. Any value is allowed, but most do not trigger any special functionality.\n`title` `string`\nOptional. The title of the schema.\n`description` `string`\nOptional. A brief description of the parameter. This could contain examples of use. Parameter description may be formatted as Markdown.\n`nullable` `boolean`\nOptional. Indicates if the value may be null.\n`enum[]` `string`\nOptional. Possible values of the element of Type.STRING with enum format. For example we can define an Enum Direction as : {type:STRING, format:enum, enum:[\"EAST\", NORTH\", \"SOUTH\", \"WEST\"]}\n`maxItems` `string (int64[](https://developers.google.com/discovery/v1/type-format) format)`\nOptional. Maximum number of the elements for Type.ARRAY.\n`minItems` `string (int64[](https://developers.google.com/discovery/v1/type-format) format)`\nOptional. Minimum number of the elements for Type.ARRAY.\n`properties` `map (key: string, value: object ())`\nOptional. Properties of Type.OBJECT.\nAn object containing a list of `\"key\": value` pairs. Example: `{ \"name\": \"wrench\", \"mass\": \"1.3kg\", \"count\": \"3\" }`.\n`required[]` `string`\nOptional. Required properties of Type.OBJECT.\n`minProperties` `string (int64[](https://developers.google.com/discovery/v1/type-format) format)`\nOptional. Minimum number of the properties for Type.OBJECT.\n`maxProperties` `string (int64[](https://developers.google.com/discovery/v1/type-format) format)`\nOptional. Maximum number of the properties for Type.OBJECT.\n`minLength` `string (int64[](https://developers.google.com/discovery/v1/type-format) format)`\nOptional. SCHEMA FIELDS FOR TYPE STRING Minimum length of the Type.STRING\n`maxLength` `string (int64[](https://developers.google.com/discovery/v1/type-format) format)`\nOptional. Maximum length of the Type.STRING\n`pattern` `string`\nOptional. Pattern of the Type.STRING to restrict a string to a regular expression.\n`example` `value ( format)`\nOptional. Example of the object. Will only populated when the object is the root.\n`anyOf[]` `object ()`\nOptional. The value should be validated against any (one or more) of the subschemas in the list.\n`propertyOrdering[]` `string`\nOptional. The order of the properties. Not a standard field in open api spec. Used to determine the order of the properties in the response.\n`default` `value ( format)`\nOptional. Default value of the field. Per JSON Schema, this field is intended for documentation generators and doesn't affect validation. Thus it's included here and ignored so that developers who send schemas with a `default` field don't get unknown-field errors.\n`items` `object ()`\nOptional. Schema of the elements of Type.ARRAY.\n`minimum` `number`\nOptional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER Minimum value of the Type.INTEGER and Type.NUMBER\n`maximum` `number`\nOptional. Maximum value of the Type.INTEGER and Type.NUMBER\nJSON representation  \n---  \n```\n{\n  \"type\": enum (),\n  \"format\": string,\n  \"title\": string,\n  \"description\": string,\n  \"nullable\": boolean,\n  \"enum\": [\n    string\n  ],\n  \"maxItems\": string,\n  \"minItems\": string,\n  \"properties\": {\n    string: {\n      object ()\n    },\n    ...\n  },\n  \"required\": [\n    string\n  ],\n  \"minProperties\": string,\n  \"maxProperties\": string,\n  \"minLength\": string,\n  \"maxLength\": string,\n  \"pattern\": string,\n  \"example\": value,\n  \"anyOf\": [\n    {\n      object ()\n    }\n  ],\n  \"propertyOrdering\": [\n    string\n  ],\n  \"default\": value,\n  \"items\": {\n    object ()\n  },\n  \"minimum\": number,\n  \"maximum\": number\n}\n```\n  \n## Type\nType contains the list of OpenAPI data types as defined by \nEnums  \n---  \n`TYPE_UNSPECIFIED` | Not specified, should not be used.  \n`STRING` | String type.  \n`NUMBER` | Number type.  \n`INTEGER` | Integer type.  \n`BOOLEAN` | Boolean type.  \n`ARRAY` | Array type.  \n`OBJECT` | Object type.  \n`NULL` | Null type.  \n## Behavior\nDefines the function behavior. Defaults to `BLOCKING`.\nEnums  \n---  \n`UNSPECIFIED` | This value is unused.  \n`BLOCKING` | If set, the system will wait to receive the function response before continuing the conversation.  \n`NON_BLOCKING` | If set, the system will not wait to receive the function response. Instead, it will attempt to handle function responses as they become available while maintaining the conversation between the user and the model.  \n## GoogleSearchRetrieval\nTool to retrieve public web data for grounding, powered by Google.\nFields \n`dynamicRetrievalConfig` `object (`DynamicRetrievalConfig[](https://ai.google.dev/api/caching#DynamicRetrievalConfig)`)`\nSpecifies the dynamic retrieval configuration for the given source.\nJSON representation  \n---  \n```\n{\n  \"dynamicRetrievalConfig\": {\n    object (DynamicRetrievalConfig[](https://ai.google.dev/api/caching#DynamicRetrievalConfig))\n  }\n}\n```\n  \n## DynamicRetrievalConfig\nDescribes the options to customize dynamic retrieval.\nFields \n`mode` `enum ()`\nThe mode of the predictor to be used in dynamic retrieval.\n`dynamicThreshold` `number`\nThe threshold to be used in dynamic retrieval. If not set, a system default value is used.\nJSON representation  \n---  \n```\n{\n  \"mode\": enum (),\n  \"dynamicThreshold\": number\n}\n```\n  \n## Mode\nThe mode of the predictor to be used in dynamic retrieval.\nEnums  \n---  \n`MODE_UNSPECIFIED` | Always trigger retrieval.  \n`MODE_DYNAMIC` | Run retrieval only when system decides it is necessary.  \n## CodeExecution\nThis type has no fields.\nTool that executes code generated by the model, and automatically returns the result to the model.\nSee also `ExecutableCode` and `CodeExecutionResult` which are only generated when using this tool.\n## GoogleSearch\nGoogleSearch tool type. Tool to support Google Search in Model. Powered by Google.\nFields \n`timeRangeFilter` `object ()`\nOptional. Filter search results to a specific time range. If customers set a start time, they must set an end time (and vice versa).\nJSON representation  \n---  \n```\n{\n  \"timeRangeFilter\": {\n    object ()\n  }\n}\n```\n  \n## Interval\nRepresents a time interval, encoded as a Timestamp start (inclusive) and a Timestamp end (exclusive).\nThe start must be less than or equal to the end. When the start equals the end, the interval is empty (matches no time). When both start and end are unspecified, the interval matches any time.\nFields \n`startTime` `string ( format)`\nOptional. Inclusive start of the interval.\nIf specified, a Timestamp matching this interval will have to be the same or after the start.\nUses RFC 3339, where generated output will always be Z-normalized and use 0, 3, 6 or 9 fractional digits. Offsets other than \"Z\" are also accepted. Examples: `\"2014-10-02T15:01:23Z\"`, `\"2014-10-02T15:01:23.045123456Z\"` or `\"2014-10-02T15:01:23+05:30\"`.\n`endTime` `string ( format)`\nOptional. Exclusive end of the interval.\nIf specified, a Timestamp matching this interval will have to be before the end.\nUses RFC 3339, where generated output will always be Z-normalized and use 0, 3, 6 or 9 fractional digits. Offsets other than \"Z\" are also accepted. Examples: `\"2014-10-02T15:01:23Z\"`, `\"2014-10-02T15:01:23.045123456Z\"` or `\"2014-10-02T15:01:23+05:30\"`.\nJSON representation  \n---  \n```\n{\n  \"startTime\": string,\n  \"endTime\": string\n}\n```\n  \n## ComputerUse\nComputer Use tool type.\nFields \n`environment` `enum ()`\nRequired. The environment being operated.\n`excludedPredefinedFunctions[]` `string`\nOptional. By default, predefined functions are included in the final model call. Some of them can be explicitly excluded from being automatically included. This can serve two purposes: 1. Using a more restricted / different action space. 2. Improving the definitions / instructions of predefined functions.\nJSON representation  \n---  \n```\n{\n  \"environment\": enum (),\n  \"excludedPredefinedFunctions\": [\n    string\n  ]\n}\n```\n  \n## Environment\nRepresents the environment being operated, such as a web browser.\nEnums  \n---  \n`ENVIRONMENT_UNSPECIFIED` | Defaults to browser.  \n`ENVIRONMENT_BROWSER` | Operates in a web browser.  \n## UrlContext\nThis type has no fields.\nTool to support URL context retrieval.\n## FileSearch\nThe FileSearch tool that retrieves knowledge from Semantic Retrieval corpora. Files are imported to Semantic Retrieval corpora using the ImportFile API.\nFields \n`fileSearchStoreNames[]` `string`\nRequired. The names of the fileSearchStores to retrieve from. Example: `fileSearchStores/my-file-search-store-123`\n`metadataFilter` `string`\nOptional. Metadata filter to apply to the semantic retrieval documents and chunks.\n`topK` `integer`\nOptional. The number of semantic retrieval chunks to retrieve.\nJSON representation  \n---  \n```\n{\n  \"fileSearchStoreNames\": [\n    string\n  ],\n  \"metadataFilter\": string,\n  \"topK\": integer\n}\n```\n  \n## GoogleMaps\nThe GoogleMaps Tool that provides geospatial context for the user's query.\nFields \n`enableWidget` `boolean`\nOptional. Whether to return a widget context token in the GroundingMetadata of the response. Developers can use the widget context token to render a Google Maps widget with geospatial context related to the places that the model references in the response.\nJSON representation  \n---  \n```\n{\n  \"enableWidget\": boolean\n}\n```\n  \n## ToolConfig\nThe Tool configuration containing parameters for specifying `Tool` use in the request.\nFields \n`functionCallingConfig` `object (`FunctionCallingConfig[](https://ai.google.dev/api/caching#FunctionCallingConfig)`)`\nOptional. Function calling config.\n`retrievalConfig` `object (`RetrievalConfig[](https://ai.google.dev/api/caching#RetrievalConfig)`)`\nOptional. Retrieval config.\nJSON representation  \n---  \n```\n{\n  \"functionCallingConfig\": {\n    object (FunctionCallingConfig[](https://ai.google.dev/api/caching#FunctionCallingConfig))\n  },\n  \"retrievalConfig\": {\n    object (RetrievalConfig[](https://ai.google.dev/api/caching#RetrievalConfig))\n  }\n}\n```\n  \n## FunctionCallingConfig\nConfiguration for specifying function calling behavior.\nFields \n`mode` `enum ()`\nOptional. Specifies the mode in which function calling should execute. If unspecified, the default value will be set to AUTO.\n`allowedFunctionNames[]` `string`\nOptional. A set of function names that, when provided, limits the functions the model will call.\nThis should only be set when the Mode is ANY or VALIDATED. Function names should match [FunctionDeclaration.name]. When set, model will predict a function call from only allowed function names.\nJSON representation  \n---  \n```\n{\n  \"mode\": enum (),\n  \"allowedFunctionNames\": [\n    string\n  ]\n}\n```\n  \n## Mode\nDefines the execution behavior for function calling by defining the execution mode.\nEnums  \n---  \n`MODE_UNSPECIFIED` | Unspecified function calling mode. This value should not be used.  \n`AUTO` | Default model behavior, model decides to predict either a function call or a natural language response.  \n`ANY` | Model is constrained to always predicting a function call only. If \"allowedFunctionNames\" are set, the predicted function call will be limited to any one of \"allowedFunctionNames\", else the predicted function call will be any one of the provided \"functionDeclarations\".  \n`NONE` | Model will not predict any function call. Model behavior is same as when not passing any function declarations.  \n`VALIDATED` | Model decides to predict either a function call or a natural language response, but will validate function calls with constrained decoding. If \"allowedFunctionNames\" are set, the predicted function call will be limited to any one of \"allowedFunctionNames\", else the predicted function call will be any one of the provided \"functionDeclarations\".  \n## RetrievalConfig\nRetrieval config.\nFields \n`latLng` `object ()`\nOptional. The location of the user.\n`languageCode` `string`\nOptional. The language code of the user. Language code for content. Use language tags defined by [BCP47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt).\nJSON representation  \n---  \n```\n{\n  \"latLng\": {\n    object ()\n  },\n  \"languageCode\": string\n}\n```\n  \n## LatLng\nAn object that represents a latitude/longitude pair. This is expressed as a pair of doubles to represent degrees latitude and degrees longitude. Unless specified otherwise, this object must conform to the [ WGS84 standard](https://en.wikipedia.org/wiki/World_Geodetic_System#1984_version). Values must be within normalized ranges.\nFields \n`latitude` `number`\nThe latitude in degrees. It must be in the range [-90.0, +90.0].\n`longitude` `number`\nThe longitude in degrees. It must be in the range [-180.0, +180.0].\nJSON representation  \n---  \n```\n{\n  \"latitude\": number,\n  \"longitude\": number\n}\n```\n  \n## UsageMetadata\nMetadata on the usage of the cached content.\nFields \n`totalTokenCount` `integer`\nTotal number of tokens that the cached content consumes.\nJSON representation  \n---  \n```\n{\n  \"totalTokenCount\": integer\n}\n```\n  \nSend feedback \nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-21 UTC.\n",
  "success": true
},
{
  "url": "https://ai.google.dev/gemini-api/docs/function-calling?example=meeting",
  "filter": "fit",
  "query": null,
  "cache": "0",
  "markdown": "[ Skip to main content ](https://ai.google.dev/gemini-api/docs/function-calling?example=meeting#main-content)\n  * English\n  * Deutsch\n  * Español – América Latina\n  * Français\n  * Indonesia\n  * Italiano\n  * Português – Brasil\n  * Tiếng Việt\n  * Русский\n  * العربيّة\n  * ภาษาไทย\n  * 中文 – 简体\n  * 中文 – 繁體\n\n[ Get API key ](https://aistudio.google.com/apikey) [ Cookbook ](https://github.com/google-gemini/cookbook) [ Community ](https://discuss.ai.google.dev/c/gemini-api/) Sign in\n\n\nSend feedback \n#  Function calling with the Gemini API \nFunction calling lets you connect models to external tools and APIs. Instead of generating text responses, the model determines when to call specific functions and provides the necessary parameters to execute real-world actions. This allows the model to act as a bridge between natural language and real-world actions and data. Function calling has 3 primary use cases:\n  * **Augment Knowledge:** Access information from external sources like databases, APIs, and knowledge bases.\n  * **Extend Capabilities:** Use external tools to perform computations and extend the limitations of the model, such as using a calculator or creating charts.\n  * **Take Actions:** Interact with external systems using APIs, such as scheduling appointments, creating invoices, sending emails, or controlling smart home devices.\n\n\nGet Weather Schedule Meeting Create Chart\n### Python\n```\nfromgoogleimport genai\nfromgoogle.genaiimport types\n\n# Define the function declaration for the model\nschedule_meeting_function = {\n    \"name\": \"schedule_meeting\",\n    \"description\": \"Schedules a meeting with specified attendees at a given time and date.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"attendees\": {\n                \"type\": \"array\",\n                \"items\": {\"type\": \"string\"},\n                \"description\": \"List of people attending the meeting.\",\n            },\n            \"date\": {\n                \"type\": \"string\",\n                \"description\": \"Date of the meeting (e.g., '2024-07-29')\",\n            },\n            \"time\": {\n                \"type\": \"string\",\n                \"description\": \"Time of the meeting (e.g., '15:00')\",\n            },\n            \"topic\": {\n                \"type\": \"string\",\n                \"description\": \"The subject or topic of the meeting.\",\n            },\n        },\n        \"required\": [\"attendees\", \"date\", \"time\", \"topic\"],\n    },\n}\n\n# Configure the client and tools\nclient = genai.Client()\ntools = types.Tool(function_declarations=[schedule_meeting_function])\nconfig = types.GenerateContentConfig(tools=[tools])\n\n# Send request with function declarations\nresponse = client.models.generate_content(\n    model=\"gemini-3-flash-preview\",\n    contents=\"Schedule a meeting with Bob and Alice for 03/14/2025 at 10:00 AM about the Q3 planning.\",\n    config=config,\n)\n\n# Check for a function call\nif response.candidates[0].content.parts[0].function_call:\n    function_call = response.candidates[0].content.parts[0].function_call\n    print(f\"Function to call: {function_call.name}\")\n    print(f\"Arguments: {function_call.args}\")\n    #  In a real app, you would call your function here:\n    #  result = schedule_meeting(**function_call.args)\nelse:\n    print(\"No function call found in the response.\")\n    print(response.text)\n\n```\n\n### JavaScript\n```\nimport{GoogleGenAI,Type}from'@google/genai';\n\n// Configure the client\nconstai=newGoogleGenAI({});\n\n// Define the function declaration for the model\nconstscheduleMeetingFunctionDeclaration={\nname:'schedule_meeting',\ndescription:'Schedules a meeting with specified attendees at a given time and date.',\nparameters:{\ntype:Type.OBJECT,\nproperties:{\nattendees:{\ntype:Type.ARRAY,\nitems:{type:Type.STRING},\ndescription:'List of people attending the meeting.',\n},\ndate:{\ntype:Type.STRING,\ndescription:'Date of the meeting (e.g., \"2024-07-29\")',\n},\ntime:{\ntype:Type.STRING,\ndescription:'Time of the meeting (e.g., \"15:00\")',\n},\ntopic:{\ntype:Type.STRING,\ndescription:'The subject or topic of the meeting.',\n},\n},\nrequired:['attendees','date','time','topic'],\n},\n};\n\n// Send request with function declarations\nconstresponse=awaitai.models.generateContent({\nmodel:'gemini-3-flash-preview',\ncontents:'Schedule a meeting with Bob and Alice for 03/27/2025 at 10:00 AM about the Q3 planning.',\nconfig:{\ntools:[{\nfunctionDeclarations:[scheduleMeetingFunctionDeclaration]\n}],\n},\n});\n\n// Check for function calls in the response\nif(response.functionCallsresponse.functionCalls.length0){\nconstfunctionCall=response.functionCalls[0];// Assuming one function call\nconsole.log(`Function to call: ${functionCall.name}`);\nconsole.log(`Arguments: ${JSON.stringify(functionCall.args)}`);\n// In a real app, you would call your actual function here:\n// const result = await scheduleMeeting(functionCall.args);\n}else{\nconsole.log(\"No function call found in the response.\");\nconsole.log(response.text);\n}\n\n```\n\n### REST\n```\ncurl\"https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent\"\\\n-H\"x-goog-api-key: $GEMINI_API_KEY\"\\\n-H'Content-Type: application/json'\\\n-XPOST\\\n-d'{\n    \"contents\": [\n\n        \"role\": \"user\",\n        \"parts\": [\n\n            \"text\": \"Schedule a meeting with Bob and Alice for 03/27/2025 at 10:00 AM about the Q3 planning.\"\n\n\n\n    ],\n    \"tools\": [\n\n        \"functionDeclarations\": [\n\n            \"name\": \"schedule_meeting\",\n            \"description\": \"Schedules a meeting with specified attendees at a given time and date.\",\n            \"parameters\": {\n              \"type\": \"object\",\n              \"properties\": {\n                \"attendees\": {\n                  \"type\": \"array\",\n                  \"items\": {\"type\": \"string\"},\n                  \"description\": \"List of people attending the meeting.\"\n\n                \"date\": {\n                  \"type\": \"string\",\n                  \"description\": \"Date of the meeting (e.g., '2024-07-29')\"\n\n                \"time\": {\n                  \"type\": \"string\",\n                  \"description\": \"Time of the meeting (e.g., '15:00')\"\n\n                \"topic\": {\n                  \"type\": \"string\",\n                  \"description\": \"The subject or topic of the meeting.\"\n\n\n              \"required\": [\"attendees\", \"date\", \"time\", \"topic\"]\n\n\n\n\n\n  }'\n\n```\n\n## How function calling works\nFunction calling involves a structured interaction between your application, the model, and external functions. Here's a breakdown of the process:\n  1. **Define Function Declaration:** Define the function declaration in your application code. Function Declarations describe the function's name, parameters, and purpose to the model.\n  2. **Call LLM with function declarations:** Send user prompt along with the function declaration(s) to the model. It analyzes the request and determines if a function call would be helpful. If so, it responds with a structured JSON object.\n  3. **Execute Function Code (Your Responsibility):** The Model _does not_ execute the function itself. It's your application's responsibility to process the response and check for Function Call, if \n     * **Yes** : Extract the name and args of the function and execute the corresponding function in your application.\n     * **No:** The model has provided a direct text response to the prompt (this flow is less emphasized in the example but is a possible outcome).\n  4. **Create User friendly response:** If a function was executed, capture the result and send it back to the model in a subsequent turn of the conversation. It will use the result to generate a final, user-friendly response that incorporates the information from the function call.\n\n\nThis process can be repeated over multiple turns, allowing for complex interactions and workflows. The model also supports calling multiple functions in a single turn ([parallel function calling](https://ai.google.dev/gemini-api/docs/function-calling#parallel_function_calling)) and in sequence ([compositional function calling](https://ai.google.dev/gemini-api/docs/function-calling#compositional_function_calling)).\n### Step 1: Define a function declaration\nDefine a function and its declaration within your application code that allows users to set light values and make an API request. This function could call external services or APIs.\n### Python\n```\n# Define a function that the model can call to control smart lights\nset_light_values_declaration = {\n    \"name\": \"set_light_values\",\n    \"description\": \"Sets the brightness and color temperature of a light.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"brightness\": {\n                \"type\": \"integer\",\n                \"description\": \"Light level from 0 to 100. Zero is off and 100 is full brightness\",\n            },\n            \"color_temp\": {\n                \"type\": \"string\",\n                \"enum\": [\"daylight\", \"cool\", \"warm\"],\n                \"description\": \"Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\",\n            },\n        },\n        \"required\": [\"brightness\", \"color_temp\"],\n    },\n}\n\n# This is the actual function that would be called based on the model's suggestion\ndefset_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:\n\"\"\"Set the brightness and color temperature of a room light. (mock API).\n\n    Args:\n        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness\n        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n\n    Returns:\n        A dictionary containing the set brightness and color temperature.\n    \"\"\"\n    return {\"brightness\": brightness, \"colorTemperature\": color_temp}\n\n```\n\n### JavaScript\n```\nimport{Type}from'@google/genai';\n\n// Define a function that the model can call to control smart lights\nconstsetLightValuesFunctionDeclaration={\nname:'set_light_values',\ndescription:'Sets the brightness and color temperature of a light.',\nparameters:{\ntype:Type.OBJECT,\nproperties:{\nbrightness:{\ntype:Type.NUMBER,\ndescription:'Light level from 0 to 100. Zero is off and 100 is full brightness',\n},\ncolor_temp:{\ntype:Type.STRING,\nenum:['daylight','cool','warm'],\ndescription:'Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.',\n},\n},\nrequired:['brightness','color_temp'],\n},\n};\n\n/**\n\n*   Set the brightness and color temperature of a room light. (mock API)\n*   @param {number} brightness - Light level from 0 to 100. Zero is off and 100 is full brightness\n*   @param {string} color_temp - Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n*   @return {Object} A dictionary containing the set brightness and color temperature.\n*/\nfunctionsetLightValues(brightness,color_temp){\nreturn{\nbrightness:brightness,\ncolorTemperature:color_temp\n};\n}\n\n```\n\n### Step 2: Call the model with function declarations\nOnce you have defined your function declarations, you can prompt the model to use them. It analyzes the prompt and function declarations and decides whether to respond directly or to call a function. If a function is called, the response object will contain a function call suggestion.\n### Python\n```\nfromgoogle.genaiimport types\n\n# Configure the client and tools\nclient = genai.Client()\ntools = types.Tool(function_declarations=[set_light_values_declaration])\nconfig = types.GenerateContentConfig(tools=[tools])\n\n# Define user prompt\ncontents = [\n    types.Content(\n        role=\"user\", parts=[types.Part(text=\"Turn the lights down to a romantic level\")]\n    )\n]\n\n# Send request with function declarations\nresponse = client.models.generate_content(\n    model=\"gemini-3-flash-preview\",\n    contents=contents,\n    config=config,\n)\n\nprint(response.candidates[0].content.parts[0].function_call)\n\n```\n\n### JavaScript\n```\nimport{GoogleGenAI}from'@google/genai';\n\n// Generation config with function declaration\nconstconfig={\ntools:[{\nfunctionDeclarations:[setLightValuesFunctionDeclaration]\n}]\n};\n\n// Configure the client\nconstai=newGoogleGenAI({});\n\n// Define user prompt\nconstcontents=[\n{\nrole:'user',\nparts:[{text:'Turn the lights down to a romantic level'}]\n}\n];\n\n// Send request with function declarations\nconstresponse=awaitai.models.generateContent({\nmodel:'gemini-3-flash-preview',\ncontents:contents,\nconfig:config\n});\n\nconsole.log(response.functionCalls[0]);\n\n```\n\nThe model then returns a `functionCall` object in an OpenAPI compatible schema specifying how to call one or more of the declared functions in order to respond to the user's question.\n### Python\n```\nid=None args={'color_temp': 'warm', 'brightness': 25} name='set_light_values'\n\n```\n\n### JavaScript\n```\n{\nname:'set_light_values',\nargs:{brightness:25,color_temp:'warm'}\n}\n\n```\n\n### Step 3: Execute set_light_values function code\nExtract the function call details from the model's response, parse the arguments , and execute the `set_light_values` function.\n### Python\n```\n# Extract tool call details, it may not be in the first part.\ntool_call = response.candidates[0].content.parts[0].function_call\n\nif tool_call.name == \"set_light_values\":\n    result = set_light_values(**tool_call.args)\n    print(f\"Function execution result: {result}\")\n\n```\n\n### JavaScript\n```\n// Extract tool call details\nconsttool_call=response.functionCalls[0]\n\nletresult;\nif(tool_call.name==='set_light_values'){\nresult=setLightValues(tool_call.args.brightness,tool_call.args.color_temp);\nconsole.log(`Function execution result: ${JSON.stringify(result)}`);\n}\n\n```\n\n### Step 4: Create user friendly response with function result and call the model again\nFinally, send the result of the function execution back to the model so it can incorporate this information into its final response to the user.\n### Python\n```\nfromgoogleimport genai\nfromgoogle.genaiimport types\n\n# Create a function response part\nfunction_response_part = types.Part.from_function_response(\n    name=tool_call.name,\n    response={\"result\": result},\n)\n\n# Append function call and result of the function execution to contents\ncontents.append(response.candidates[0].content) # Append the content from the model's response.\ncontents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n\nclient = genai.Client()\nfinal_response = client.models.generate_content(\n    model=\"gemini-3-flash-preview\",\n    config=config,\n    contents=contents,\n)\n\nprint(final_response.text)\n\n```\n\n### JavaScript\n```\n// Create a function response part\nconstfunction_response_part={\nname:tool_call.name,\nresponse:{result}\n}\n\n// Append function call and result of the function execution to contents\ncontents.push(response.candidates[0].content);\ncontents.push({role:'user',parts:[{functionResponse:function_response_part}]});\n\n// Get the final response from the model\nconstfinal_response=awaitai.models.generateContent({\nmodel:'gemini-3-flash-preview',\ncontents:contents,\nconfig:config\n});\n\nconsole.log(final_response.text);\n\n```\n\nThis completes the function calling flow. The model successfully used the `set_light_values` function to perform the request action of the user.\n## Function declarations\nWhen you implement function calling in a prompt, you create a `tools` object, which contains one or more `function declarations`. You define functions using JSON, specifically with a [select subset](https://ai.google.dev/api/caching#Schema) of the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schemaw) format. A single function declaration can include the following parameters:\n  * `name` (string): A unique name for the function (`get_weather_forecast`, `send_email`). Use descriptive names without spaces or special characters (use underscores or camelCase).\n  * `description` (string): A clear and detailed explanation of the function's purpose and capabilities. This is crucial for the model to understand when to use the function. Be specific and provide examples if helpful (\"Finds theaters based on location and optionally movie title which is currently playing in theaters.\").\n  * `parameters` (object): Defines the input parameters the function expects. \n    * `type` (string): Specifies the overall data type, such as `object`.\n    * `properties` (object): Lists individual parameters, each with: \n      * `type` (string): The data type of the parameter, such as `string`, `integer`, `boolean, array`.\n      * `description` (string): A description of the parameter's purpose and format. Provide examples and constraints (\"The city and state, e.g., 'San Francisco, CA' or a zip code e.g., '95616'.\").\n      * `enum` (array, optional): If the parameter values are from a fixed set, use \"enum\" to list the allowed values instead of just describing them in the description. This improves accuracy (\"enum\": [\"daylight\", \"cool\", \"warm\"]).\n    * `required` (array): An array of strings listing the parameter names that are mandatory for the function to operate.\n\n\nYou can also construct `FunctionDeclarations` from Python functions directly using `types.FunctionDeclaration.from_callable(client=client, callable=your_function)`.\n## Function calling with thinking models\nGemini 3 and 2.5 series models use an internal [\"thinking\"](https://ai.google.dev/gemini-api/docs/thinking) process to reason through requests. This significantly improves function calling performance, allowing the model to better determine when to call a function and which parameters to use. Because the Gemini API is stateless, models use [thought signatures](https://ai.google.dev/gemini-api/docs/thought-signatures) to maintain context across multi-turn conversations.\nThis section covers advanced management of thought signatures and is only necessary if you're manually constructing API requests (e.g., via REST) or manipulating conversation history.\n**If you're using the[Google GenAI SDKs](https://ai.google.dev/gemini-api/docs/libraries) (our official libraries), you don't need to manage this process**. The SDKs automatically handle the necessary steps, as shown in the earlier [example](https://ai.google.dev/gemini-api/docs/function-calling#step-4).\n### Managing conversation history manually\nIf you modify the conversation history manually, instead of sending the [complete previous response](https://ai.google.dev/gemini-api/docs/function-calling#step-4) you must correctly handle the `thought_signature` included in the model's turn.\nFollow these rules to ensure the model's context is preserved:\n  * Always send the `thought_signature` back to the model inside its original [`Part`](https://ai.google.dev/api#request-body-structure).\n  * Don't merge a `Part` containing a signature with one that does not. This breaks the positional context of the thought.\n  * Don't combine two `Parts` that both contain signatures, as the signature strings cannot be merged.\n\n\n#### Gemini 3 thought signatures\nIn Gemini 3, any [`Part`](https://ai.google.dev/api#request-body-structure) of a model response may contain a thought signature. While we generally recommend returning signatures from all `Part` types, passing back thought signatures is mandatory for function calling. Unless you are manipulating conversation history manually, the Google GenAI SDK will handle thought signatures automatically.\nIf you are manipulating conversation history manually, refer to the [Thoughts Signatures](https://ai.google.dev/gemini-api/docs/thought-signatures) page for complete guidance and details on handling thought signatures for Gemini 3.\n### Inspecting thought signatures\nWhile not necessary for implementation, you can inspect the response to see the `thought_signature` for debugging or educational purposes.\n### Python\n```\nimportbase64\n# After receiving a response from a model with thinking enabled\n# response = client.models.generate_content(...)\n\n# The signature is attached to the response part containing the function call\npart = response.candidates[0].content.parts[0]\nif part.thought_signature:\n  print(base64.b64encode(part.thought_signature).decode(\"utf-8\"))\n\n```\n\n### JavaScript\n```\n// After receiving a response from a model with thinking enabled\n// const response = await ai.models.generateContent(...)\n\n// The signature is attached to the response part containing the function call\nconstpart=response.candidates[0].content.parts[0];\nif(part.thoughtSignature){\nconsole.log(part.thoughtSignature);\n}\n\n```\n\nLearn more about limitations and usage of thought signatures, and about thinking models in general, on the [Thinking](https://ai.google.dev/gemini-api/docs/thinking#signatures) page.\n## Parallel function calling\nIn addition to single turn function calling, you can also call multiple functions at once. Parallel function calling lets you execute multiple functions at once and is used when the functions are not dependent on each other. This is useful in scenarios like gathering data from multiple independent sources, such as retrieving customer details from different databases or checking inventory levels across various warehouses or performing multiple actions such as converting your apartment into a disco.\n### Python\n```\npower_disco_ball = {\n    \"name\": \"power_disco_ball\",\n    \"description\": \"Powers the spinning disco ball.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"power\": {\n                \"type\": \"boolean\",\n                \"description\": \"Whether to turn the disco ball on or off.\",\n            }\n        },\n        \"required\": [\"power\"],\n    },\n}\n\nstart_music = {\n    \"name\": \"start_music\",\n    \"description\": \"Play some music matching the specified parameters.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"energetic\": {\n                \"type\": \"boolean\",\n                \"description\": \"Whether the music is energetic or not.\",\n            },\n            \"loud\": {\n                \"type\": \"boolean\",\n                \"description\": \"Whether the music is loud or not.\",\n            },\n        },\n        \"required\": [\"energetic\", \"loud\"],\n    },\n}\n\ndim_lights = {\n    \"name\": \"dim_lights\",\n    \"description\": \"Dim the lights.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"brightness\": {\n                \"type\": \"number\",\n                \"description\": \"The brightness of the lights, 0.0 is off, 1.0 is full.\",\n            }\n        },\n        \"required\": [\"brightness\"],\n    },\n}\n\n```\n\n### JavaScript\n```\nimport{Type}from'@google/genai';\n\nconstpowerDiscoBall={\nname:'power_disco_ball',\ndescription:'Powers the spinning disco ball.',\nparameters:{\ntype:Type.OBJECT,\nproperties:{\npower:{\ntype:Type.BOOLEAN,\ndescription:'Whether to turn the disco ball on or off.'\n}\n},\nrequired:['power']\n}\n};\n\nconststartMusic={\nname:'start_music',\ndescription:'Play some music matching the specified parameters.',\nparameters:{\ntype:Type.OBJECT,\nproperties:{\nenergetic:{\ntype:Type.BOOLEAN,\ndescription:'Whether the music is energetic or not.'\n},\nloud:{\ntype:Type.BOOLEAN,\ndescription:'Whether the music is loud or not.'\n}\n},\nrequired:['energetic','loud']\n}\n};\n\nconstdimLights={\nname:'dim_lights',\ndescription:'Dim the lights.',\nparameters:{\ntype:Type.OBJECT,\nproperties:{\nbrightness:{\ntype:Type.NUMBER,\ndescription:'The brightness of the lights, 0.0 is off, 1.0 is full.'\n}\n},\nrequired:['brightness']\n}\n};\n\n```\n\nConfigure the function calling mode to allow using all of the specified tools. To learn more, you can read about [configuring function calling](https://ai.google.dev/gemini-api/docs/function-calling#function_calling_modes).\n### Python\n```\nfromgoogleimport genai\nfromgoogle.genaiimport types\n\n# Configure the client and tools\nclient = genai.Client()\nhouse_tools = [\n    types.Tool(function_declarations=[power_disco_ball, start_music, dim_lights])\n]\nconfig = types.GenerateContentConfig(\n    tools=house_tools,\n    automatic_function_calling=types.AutomaticFunctionCallingConfig(\n        disable=True\n    ),\n    # Force the model to call 'any' function, instead of chatting.\n    tool_config=types.ToolConfig(\n        function_calling_config=types.FunctionCallingConfig(mode='ANY')\n    ),\n)\n\nchat = client.chats.create(model=\"gemini-3-flash-preview\", config=config)\nresponse = chat.send_message(\"Turn this place into a party!\")\n\n# Print out each of the function calls requested from this single call\nprint(\"Example 1: Forced function calling\")\nfor fn in response.function_calls:\n    args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n    print(f\"{fn.name}({args})\")\n\n```\n\n### JavaScript\n```\nimport{GoogleGenAI}from'@google/genai';\n\n// Set up function declarations\nconsthouseFns=[powerDiscoBall,startMusic,dimLights];\n\nconstconfig={\ntools:[{\nfunctionDeclarations:houseFns\n}],\n// Force the model to call 'any' function, instead of chatting.\ntoolConfig:{\nfunctionCallingConfig:{\nmode:'any'\n}\n}\n};\n\n// Configure the client\nconstai=newGoogleGenAI({});\n\n// Create a chat session\nconstchat=ai.chats.create({\nmodel:'gemini-3-flash-preview',\nconfig:config\n});\nconstresponse=awaitchat.sendMessage({message:'Turn this place into a party!'});\n\n// Print out each of the function calls requested from this single call\nconsole.log(\"Example 1: Forced function calling\");\nfor(constfnofresponse.functionCalls){\nconstargs=Object.entries(fn.args)\n.map(([key,val])=>`${key}=${val}`)\n.join(', ');\nconsole.log(`${fn.name}(${args})`);\n}\n\n```\n\nEach of the printed results reflects a single function call that the model has requested. To send the results back, include the responses in the same order as they were requested.\nThe Python SDK supports [automatic function calling](https://ai.google.dev/gemini-api/docs/function-calling#automatic_function_calling_python_only), which automatically converts Python functions to declarations, handles the function call execution and response cycle for you. Following is an example for the disco use case.\n### Python\n```\nfromgoogleimport genai\nfromgoogle.genaiimport types\n\n# Actual function implementations\ndefpower_disco_ball_impl(power: bool) -> dict:\n\"\"\"Powers the spinning disco ball.\n\n    Args:\n        power: Whether to turn the disco ball on or off.\n\n    Returns:\n        A status dictionary indicating the current state.\n    \"\"\"\n    return {\"status\": f\"Disco ball powered {'on'ifpowerelse'off'}\"}\n\ndefstart_music_impl(energetic: bool, loud: bool) -> dict:\n\"\"\"Play some music matching the specified parameters.\n\n    Args:\n        energetic: Whether the music is energetic or not.\n        loud: Whether the music is loud or not.\n\n    Returns:\n        A dictionary containing the music settings.\n    \"\"\"\n    music_type = \"energetic\" if energetic else \"chill\"\n    volume = \"loud\" if loud else \"quiet\"\n    return {\"music_type\": music_type, \"volume\": volume}\n\ndefdim_lights_impl(brightness: float) -> dict:\n\"\"\"Dim the lights.\n\n    Args:\n        brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n\n    Returns:\n        A dictionary containing the new brightness setting.\n    \"\"\"\n    return {\"brightness\": brightness}\n\n# Configure the client\nclient = genai.Client()\nconfig = types.GenerateContentConfig(\n    tools=[power_disco_ball_impl, start_music_impl, dim_lights_impl]\n)\n\n# Make the request\nresponse = client.models.generate_content(\n    model=\"gemini-3-flash-preview\",\n    contents=\"Do everything you need to this place into party!\",\n    config=config,\n)\n\nprint(\"\\nExample 2: Automatic function calling\")\nprint(response.text)\n# I've turned on the disco ball, started playing loud and energetic music, and dimmed the lights to 50% brightness. Let's get this party started!\n\n```\n\n## Compositional function calling\nCompositional or sequential function calling allows Gemini to chain multiple function calls together to fulfill a complex request. For example, to answer \"Get the temperature in my current location\", the Gemini API might first invoke a `get_current_location()` function followed by a `get_weather()` function that takes the location as a parameter.\nThe following example demonstrates how to implement compositional function calling using the Python SDK and automatic function calling.\n### Python\nThis example uses the automatic function calling feature of the `google-genai` Python SDK. The SDK automatically converts the Python functions to the required schema, executes the function calls when requested by the model, and sends the results back to the model to complete the task.\n```\nimportos\nfromgoogleimport genai\nfromgoogle.genaiimport types\n\n# Example Functions\ndefget_weather_forecast(location: str) -> dict:\n\"\"\"Gets the current weather temperature for a given location.\"\"\"\n    print(f\"Tool Call: get_weather_forecast(location={location})\")\n    # TODO: Make API call\n    print(\"Tool Response: {'temperature': 25, 'unit': 'celsius'}\")\n    return {\"temperature\": 25, \"unit\": \"celsius\"}  # Dummy response\n\ndefset_thermostat_temperature(temperature: int) -> dict:\n\"\"\"Sets the thermostat to a desired temperature.\"\"\"\n    print(f\"Tool Call: set_thermostat_temperature(temperature={temperature})\")\n    # TODO: Interact with a thermostat API\n    print(\"Tool Response: {'status': 'success'}\")\n    return {\"status\": \"success\"}\n\n# Configure the client and model\nclient = genai.Client()\nconfig = types.GenerateContentConfig(\n    tools=[get_weather_forecast, set_thermostat_temperature]\n)\n\n# Make the request\nresponse = client.models.generate_content(\n    model=\"gemini-3-flash-preview\",\n    contents=\"If it's warmer than 20°C in London, set the thermostat to 20°C, otherwise set it to 18°C.\",\n    config=config,\n)\n\n# Print the final, user-facing response\nprint(response.text)\n\n```\n\n**Expected Output**\nWhen you run the code, you will see the SDK orchestrating the function calls. The model first calls `get_weather_forecast`, receives the temperature, and then calls `set_thermostat_temperature` with the correct value based on the logic in the prompt.\n```\nTool Call: get_weather_forecast(location=London)\nTool Response: {'temperature': 25, 'unit': 'celsius'}\nTool Call: set_thermostat_temperature(temperature=20)\nTool Response: {'status': 'success'}\nOK. I've set the thermostat to 20°C.\n\n```\n\n### JavaScript\nThis example shows how to use JavaScript/TypeScript SDK to do comopositional function calling using a manual execution loop.\n```\nimport{GoogleGenAI,Type}from\"@google/genai\";\n\n// Configure the client\nconstai=newGoogleGenAI({});\n\n// Example Functions\nfunctionget_weather_forecast({location}){\nconsole.log(`Tool Call: get_weather_forecast(location=${location})`);\n// TODO: Make API call\nconsole.log(\"Tool Response: {'temperature': 25, 'unit': 'celsius'}\");\nreturn{temperature:25,unit:\"celsius\"};\n}\n\nfunctionset_thermostat_temperature({temperature}){\nconsole.log(\n`Tool Call: set_thermostat_temperature(temperature=${temperature})`,\n);\n// TODO: Make API call\nconsole.log(\"Tool Response: {'status': 'success'}\");\nreturn{status:\"success\"};\n}\n\nconsttoolFunctions={\nget_weather_forecast,\nset_thermostat_temperature,\n};\n\nconsttools=[\n{\nfunctionDeclarations:[\n{\nname:\"get_weather_forecast\",\ndescription:\n\"Gets the current weather temperature for a given location.\",\nparameters:{\ntype:Type.OBJECT,\nproperties:{\nlocation:{\ntype:Type.STRING,\n},\n},\nrequired:[\"location\"],\n},\n},\n{\nname:\"set_thermostat_temperature\",\ndescription:\"Sets the thermostat to a desired temperature.\",\nparameters:{\ntype:Type.OBJECT,\nproperties:{\ntemperature:{\ntype:Type.NUMBER,\n},\n},\nrequired:[\"temperature\"],\n},\n},\n],\n},\n];\n\n// Prompt for the model\nletcontents=[\n{\nrole:\"user\",\nparts:[\n{\ntext:\"If it's warmer than 20°C in London, set the thermostat to 20°C, otherwise set it to 18°C.\",\n},\n],\n},\n];\n\n// Loop until the model has no more function calls to make\nwhile(true){\nconstresult=awaitai.models.generateContent({\nmodel:\"gemini-3-flash-preview\",\ncontents,\nconfig:{tools},\n});\n\nif(result.functionCallsresult.functionCalls.length0){\nconstfunctionCall=result.functionCalls[0];\n\nconst{name,args}=functionCall;\n\nif(!toolFunctions[name]){\nthrownewError(`Unknown function call: ${name}`);\n}\n\n// Call the function and get the response.\nconsttoolResponse=toolFunctions[name](args);\n\nconstfunctionResponsePart={\nname:functionCall.name,\nresponse:{\nresult:toolResponse,\n},\n};\n\n// Send the function response back to the model.\ncontents.push({\nrole:\"model\",\nparts:[\n{\nfunctionCall:functionCall,\n},\n],\n});\ncontents.push({\nrole:\"user\",\nparts:[\n{\nfunctionResponse:functionResponsePart,\n},\n],\n});\n}else{\n// No more function calls, break the loop.\nconsole.log(result.text);\nbreak;\n}\n}\n\n```\n\n**Expected Output**\nWhen you run the code, you will see the SDK orchestrating the function calls. The model first calls `get_weather_forecast`, receives the temperature, and then calls `set_thermostat_temperature` with the correct value based on the logic in the prompt.\n```\nToolCall:get_weather_forecast(location=London)\nToolResponse:{'temperature':25,'unit':'celsius'}\nToolCall:set_thermostat_temperature(temperature=20)\nToolResponse:{'status':'success'}\nOK.It's 25°C in London, so I'vesetthethermostatto20°C.\n\n```\n\nCompositional function calling is a native [Live API](https://ai.google.dev/gemini-api/docs/live) feature. This means Live API can handle the function calling similar to the Python SDK.\n### Python\n```\n# Light control schemas\nturn_on_the_lights_schema = {'name': 'turn_on_the_lights'}\nturn_off_the_lights_schema = {'name': 'turn_off_the_lights'}\n\nprompt = \"\"\"\n  Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?\n  \"\"\"\n\ntools = [\n    {'code_execution': {}},\n    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]}\n]\n\nawait run(prompt, tools=tools, modality=\"AUDIO\")\n\n```\n\n### JavaScript\n```\n// Light control schemas\nconstturnOnTheLightsSchema={name:'turn_on_the_lights'};\nconstturnOffTheLightsSchema={name:'turn_off_the_lights'};\n\nconstprompt=`\n  Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?\n`;\n\nconsttools=[\n{codeExecution:{}},\n{functionDeclarations:[turnOnTheLightsSchema,turnOffTheLightsSchema]}\n];\n\nawaitrun(prompt,tools=tools,modality=\"AUDIO\")\n\n```\n\n## Function calling modes\nThe Gemini API lets you control how the model uses the provided tools (function declarations). Specifically, you can set the mode within the.`function_calling_config`.\n  * `AUTO (Default)`: The model decides whether to generate a natural language response or suggest a function call based on the prompt and context. This is the most flexible mode and recommended for most scenarios.\n  * `ANY`: The model is constrained to always predict a function call and guarantees function schema adherence. If `allowed_function_names` is not specified, the model can choose from any of the provided function declarations. If `allowed_function_names` is provided as a list, the model can only choose from the functions in that list. Use this mode when you require a function call response to every prompt (if applicable).\n  * `NONE`: The model is _prohibited_ from making function calls. This is equivalent to sending a request without any function declarations. Use this to temporarily disable function calling without removing your tool definitions.\n  * `VALIDATED` (Preview): The model is constrained to predict either function calls or natural language, and ensures function schema adherence. If `allowed_function_names` is not provided, the model picks from all of the available function declarations. If `allowed_function_names` is provided, the model picks from the set of allowed functions.\n\n\n### Python\n```\nfromgoogle.genaiimport types\n\n# Configure function calling mode\ntool_config = types.ToolConfig(\n    function_calling_config=types.FunctionCallingConfig(\n        mode=\"ANY\", allowed_function_names=[\"get_current_temperature\"]\n    )\n)\n\n# Create the generation config\nconfig = types.GenerateContentConfig(\n    tools=[tools],  # not defined here.\n    tool_config=tool_config,\n)\n\n```\n\n### JavaScript\n```\nimport{FunctionCallingConfigMode}from'@google/genai';\n\n// Configure function calling mode\nconsttoolConfig={\nfunctionCallingConfig:{\nmode:FunctionCallingConfigMode.ANY,\nallowedFunctionNames:['get_current_temperature']\n}\n};\n\n// Create the generation config\nconstconfig={\ntools:tools,// not defined here.\ntoolConfig:toolConfig,\n};\n\n```\n\n## Automatic function calling (Python only)\nWhen using the Python SDK, you can provide Python functions directly as tools. The SDK converts these functions into declarations, manages the function call execution, and handles the response cycle for you. Define your function with type hints and a docstring. For optimal results, it is recommended to use [Google-style docstrings.](https://google.github.io/styleguide/pyguide.html#383-functions-and-methods) The SDK will then automatically:\n  1. Detect function call responses from the model.\n  2. Call the corresponding Python function in your code.\n  3. Send the function's response back to the model.\n  4. Return the model's final text response.\n\n\nThe SDK currently does not parse argument descriptions into the property description slots of the generated function declaration. Instead, it sends the entire docstring as the top-level function description.\n### Python\n```\nfromgoogleimport genai\nfromgoogle.genaiimport types\n\n# Define the function with type hints and docstring\ndefget_current_temperature(location: str) -> dict:\n\"\"\"Gets the current temperature for a given location.\n\n    Args:\n        location: The city and state, e.g. San Francisco, CA\n\n    Returns:\n        A dictionary containing the temperature and unit.\n    \"\"\"\n    # ... (implementation) ...\n    return {\"temperature\": 25, \"unit\": \"Celsius\"}\n\n# Configure the client\nclient = genai.Client()\nconfig = types.GenerateContentConfig(\n    tools=[get_current_temperature]\n)  # Pass the function itself\n\n# Make the request\nresponse = client.models.generate_content(\n    model=\"gemini-3-flash-preview\",\n    contents=\"What's the temperature in Boston?\",\n    config=config,\n)\n\nprint(response.text)  # The SDK handles the function call and returns the final text\n\n```\n\nYou can disable automatic function calling with:\n### Python\n```\nconfig = types.GenerateContentConfig(\n    tools=[get_current_temperature],\n    automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)\n)\n\n```\n\n### Automatic function schema declaration\nThe API is able to describe any of the following types. `Pydantic` types are allowed, as long as the fields defined on them are also composed of allowed types. Dict types (like `dict[str: int]`) are not well supported here, don't use them.\n### Python\n```\nAllowedType = (\n  int | float | bool | str | list['AllowedType'] | pydantic.BaseModel)\n\n```\n\nTo see what the inferred schema looks like, you can convert it using [`from_callable`](https://googleapis.github.io/python-genai/genai.html#genai.types.FunctionDeclaration.from_callable):\n### Python\n```\nfromgoogleimport genai\nfromgoogle.genaiimport types\n\ndefmultiply(a: float, b: float):\n\"\"\"Returns a * b.\"\"\"\n    return a * b\n\nclient = genai.Client()\nfn_decl = types.FunctionDeclaration.from_callable(callable=multiply, client=client)\n\n# to_json_dict() provides a clean JSON representation.\nprint(fn_decl.to_json_dict())\n\n```\n\n## Multi-tool use: Combine native tools with function calling\nYou can enable multiple tools combining native tools with function calling at the same time. Here's an example that enables two tools, [Grounding with Google Search](https://ai.google.dev/gemini-api/docs/grounding) and [code execution](https://ai.google.dev/gemini-api/docs/code-execution), in a request using the [Live API](https://ai.google.dev/gemini-api/docs/live).\n### Python\n```\n# Multiple tasks example - combining lights, code execution, and search\nprompt = \"\"\"\n  Hey, I need you to do three things for me.\n\n    1.  Turn on the lights.\n    2.  Then compute the largest prime palindrome under 100000.\n    3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\n\n  Thanks!\n  \"\"\"\n\ntools = [\n    {'google_search': {}},\n    {'code_execution': {}},\n    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]} # not defined here.\n]\n\n# Execute the prompt with specified tools in audio modality\nawait run(prompt, tools=tools, modality=\"AUDIO\")\n\n```\n\n### JavaScript\n```\n// Multiple tasks example - combining lights, code execution, and search\nconstprompt=`\n  Hey, I need you to do three things for me.\n\n    1.  Turn on the lights.\n    2.  Then compute the largest prime palindrome under 100000.\n    3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\n\n  Thanks!\n`;\n\nconsttools=[\n{googleSearch:{}},\n{codeExecution:{}},\n{functionDeclarations:[turnOnTheLightsSchema,turnOffTheLightsSchema]}// not defined here.\n];\n\n// Execute the prompt with specified tools in audio modality\nawaitrun(prompt,{tools:tools,modality:\"AUDIO\"});\n\n```\n\nPython developers can try this out in the [Live API Tool Use notebook](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb).\n## Multimodal function responses\nFor Gemini 3 series models, you can include multimodal content in the function response parts that you send to the model. The model can process this multimodal content in its next turn to produce a more informed response. The following MIME types are supported for multimodal content in function responses:\n  * **Images** : `image/png`, `image/jpeg`, `image/webp`\n  * **Documents** : `application/pdf`, `text/plain`\n\n\nTo include multimodal data in a function response, include it as one or more parts nested within the `functionResponse` part. Each multimodal part must contain `inlineData`. If you reference a multimodal part from within the structured `response` field, it must contain a unique `displayName`.\nYou can also reference a multimodal part from within the structured `response` field of the `functionResponse` part by using the JSON reference format `{\"$ref\": \"\"}`. The model substitutes the reference with the multimodal content when processing the response. Each `displayName` can only be referenced once in the structured `response` field.\nThe following example shows a message containing a `functionResponse` for a function named `get_image` and a nested part containing image data with `displayName: \"instrument.jpg\"`. The `functionResponse`'s `response` field references this image part:\n###  Python \n```\nfromgoogleimport genai\nfromgoogle.genaiimport types\n\nimportrequests\n\nclient = genai.Client()\n\n# This is a manual, two turn multimodal function calling workflow:\n\n# 1. Define the function tool\nget_image_declaration = types.FunctionDeclaration(\n  name=\"get_image\",\n  description=\"Retrieves the image file reference for a specific order item.\",\n  parameters={\n      \"type\": \"object\",\n      \"properties\": {\n          \"item_name\": {\n              \"type\": \"string\",\n              \"description\": \"The name or description of the item ordered (e.g., 'instrument').\"\n          }\n      },\n      \"required\": [\"item_name\"],\n  },\n)\ntool_config = types.Tool(function_declarations=[get_image_declaration])\n\n# 2. Send a message that triggers the tool\nprompt = \"Show me the instrument I ordered last month.\"\nresponse_1 = client.models.generate_content(\n  model=\"gemini-3-flash-preview\",\n  contents=[prompt],\n  config=types.GenerateContentConfig(\n      tools=[tool_config],\n  )\n)\n\n# 3. Handle the function call\nfunction_call = response_1.function_calls[0]\nrequested_item = function_call.args[\"item_name\"]\nprint(f\"Model wants to call: {function_call.name}\")\n\n# Execute your tool (e.g., call an API)\n# (This is a mock response for the example)\nprint(f\"Calling external tool for: {requested_item}\")\n\nfunction_response_data = {\n  \"image_ref\": {\"$ref\": \"instrument.jpg\"},\n}\nimage_path = \"https://goo.gle/instrument-img\"\nimage_bytes = requests.get(image_path).content\nfunction_response_multimodal_data = types.FunctionResponsePart(\n  inline_data=types.FunctionResponseBlob(\n    mime_type=\"image/jpeg\",\n    display_name=\"instrument.jpg\",\n    data=image_bytes,\n  )\n)\n\n# 4. Send the tool's result back\n# Append this turn's messages to history for a final response.\nhistory = [\n  types.Content(role=\"user\", parts=[types.Part(text=prompt)]),\n  response_1.candidates[0].content,\n  types.Content(\n    role=\"tool\",\n    parts=[\n        types.Part.from_function_response(\n          name=function_call.name,\n          response=function_response_data,\n          parts=[function_response_multimodal_data]\n        )\n    ],\n  )\n]\n\nresponse_2 = client.models.generate_content(\n  model=\"gemini-3-flash-preview\",\n  contents=history,\n  config=types.GenerateContentConfig(\n      tools=[tool_config],\n      thinking_config=types.ThinkingConfig(include_thoughts=True)\n  ),\n)\n\nprint(f\"\\nFinal model response: {response_2.text}\")\n\n```\n\n###  JavaScript \n```\nimport{GoogleGenAI,Type}from'@google/genai';\n\nconstclient=newGoogleGenAI({apiKey:process.env.GEMINI_API_KEY});\n\n// This is a manual, two turn multimodal function calling workflow:\n// 1. Define the function tool\nconstgetImageDeclaration={\nname:'get_image',\ndescription:'Retrieves the image file reference for a specific order item.',\nparameters:{\ntype:Type.OBJECT,\nproperties:{\nitem_name:{\ntype:Type.STRING,\ndescription:\"The name or description of the item ordered (e.g., 'instrument').\",\n},\n},\nrequired:['item_name'],\n},\n};\n\nconsttoolConfig={\nfunctionDeclarations:[getImageDeclaration],\n};\n\n// 2. Send a message that triggers the tool\nconstprompt='Show me the instrument I ordered last month.';\nconstresponse1=awaitclient.models.generateContent({\nmodel:'gemini-3-flash-preview',\ncontents:prompt,\nconfig:{\ntools:[toolConfig],\n},\n});\n\n// 3. Handle the function call\nconstfunctionCall=response1.functionCalls[0];\nconstrequestedItem=functionCall.args.item_name;\nconsole.log(`Model wants to call: ${functionCall.name}`);\n\n// Execute your tool (e.g., call an API)\n// (This is a mock response for the example)\nconsole.log(`Calling external tool for: ${requestedItem}`);\n\nconstfunctionResponseData={\nimage_ref:{$ref:'instrument.jpg'},\n};\n\nconstimageUrl=\"https://goo.gle/instrument-img\";\nconstresponse=awaitfetch(imageUrl);\nconstimageArrayBuffer=awaitresponse.arrayBuffer();\nconstbase64ImageData=Buffer.from(imageArrayBuffer).toString('base64');\n\nconstfunctionResponseMultimodalData={\ninlineData:{\nmimeType:'image/jpeg',\ndisplayName:'instrument.jpg',\ndata:base64ImageData,\n},\n};\n\n// 4. Send the tool's result back\n// Append this turn's messages to history for a final response.\nconsthistory=[\n{role:'user',parts:[{text:prompt}]},\nresponse1.candidates[0].content,\n{\nrole:'tool',\nparts:[\n{\nfunctionResponse:{\nname:functionCall.name,\nresponse:functionResponseData,\nparts:[functionResponseMultimodalData],\n},\n},\n],\n},\n];\n\nconstresponse2=awaitclient.models.generateContent({\nmodel:'gemini-3-flash-preview',\ncontents:history,\nconfig:{\ntools:[toolConfig],\nthinkingConfig:{includeThoughts:true},\n},\n});\n\nconsole.log(`\\nFinal model response: ${response2.text}`);\n\n```\n\n###  REST \n```\nIMG_URL=\"https://goo.gle/instrument-img\"\n\nMIME_TYPE=$(curl-sIL\"$IMG_URL\"|grep-i'^content-type:'|awk-F': ''{print $2}'|sed's/\\r$//'|head-n1)\nif[[-z\"$MIME_TYPE\"||!\"$MIME_TYPE\"==image/*]];then\nMIME_TYPE=\"image/jpeg\"\nfi\n\n# Check for macOS\nif[[\"$(uname)\"==\"Darwin\"]];then\nIMAGE_B64=$(curl-sL\"$IMG_URL\"|base64-b0)\nelif[[\"$(base64--version2>&1)\"=*\"FreeBSD\"*]];then\nIMAGE_B64=$(curl-sL\"$IMG_URL\"|base64)\nelse\nIMAGE_B64=$(curl-sL\"$IMG_URL\"|base64-w0)\nfi\n\ncurl\"https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent\"\\\n-H\"x-goog-api-key: $GEMINI_API_KEY\"\\\n-H'Content-Type: application/json'\\\n-XPOST\\\n-d'{\n    \"contents\": [\n      ...,\n\n        \"role\": \"user\",\n        \"parts\": [\n\n            \"functionResponse\": {\n              \"name\": \"get_image\",\n              \"response\": {\n                \"image_ref\": {\n                  \"$ref\": \"instrument.jpg\"\n\n\n              \"parts\": [\n\n                  \"inlineData\": {\n                    \"displayName\": \"instrument.jpg\",\n                    \"mimeType\":\"'\"$MIME_TYPE\"'\",\n                    \"data\": \"'\"$IMAGE_B64\"'\"\n\n\n\n\n\n\n\n\n  }'\n\n```\n\n## Function calling with Structured output\nFor Gemini 3 series models, you can use function calling with [structured output](https://ai.google.dev/gemini-api/docs/structured-output). This lets the model predict function calls or outputs that adhere to a specific schema. As a result, you receive consistently formatted responses when the model doesn't generate function calls.\n## Model context protocol (MCP)\n[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open standard for connecting AI applications with external tools and data. MCP provides a common protocol for models to access context, such as functions (tools), data sources (resources), or predefined prompts.\nThe Gemini SDKs have built-in support for the MCP, reducing boilerplate code and offering [automatic tool calling](https://ai.google.dev/gemini-api/docs/function-calling#automatic_function_calling_python_only) for MCP tools. When the model generates an MCP tool call, the Python and JavaScript client SDK can automatically execute the MCP tool and send the response back to the model in a subsequent request, continuing this loop until no more tool calls are made by the model.\nHere, you can find an example of how to use a local MCP server with Gemini and `mcp` SDK.\n### Python\nMake sure the latest version of the [`mcp` SDK](https://modelcontextprotocol.io/introduction) is installed on your platform of choice.\n```\npipinstallmcp\n\n```\n```\nimportos\nimportasyncio\nfromdatetimeimport datetime\nfrommcpimport ClientSession, StdioServerParameters\nfrommcp.client.stdioimport stdio_client\nfromgoogleimport genai\n\nclient = genai.Client()\n\n# Create server parameters for stdio connection\nserver_params = StdioServerParameters(\n    command=\"npx\",  # Executable\n    args=[\"-y\", \"@philschmid/weather-mcp\"],  # MCP Server\n    env=None,  # Optional environment variables\n)\n\nasync defrun():\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            # Prompt to get the weather for the current day in London.\n            prompt = f\"What is the weather in London in {datetime.now().strftime('%Y-%m-%d')}?\"\n\n            # Initialize the connection between client and server\n            await session.initialize()\n\n            # Send request to the model with MCP function declarations\n            response = await client.aio.models.generate_content(\n                model=\"gemini-2.5-flash\",\n                contents=prompt,\n                config=genai.types.GenerateContentConfig(\n                    temperature=0,\n                    tools=[session],  # uses the session, will automatically call the tool\n                    # Uncomment if you **don't** want the SDK to automatically call the tool\n                    # automatic_function_calling=genai.types.AutomaticFunctionCallingConfig(\n                    #     disable=True\n                    # ),\n                ),\n            )\n            print(response.text)\n\n# Start the asyncio event loop and run the main function\nasyncio.run(run())\n\n```\n\n### JavaScript\nMake sure the latest version of the `mcp` SDK is installed on your platform of choice.\n```\nnpminstall@modelcontextprotocol/sdk\n\n```\n```\nimport{GoogleGenAI,FunctionCallingConfigMode,mcpToTool}from'@google/genai';\nimport{Client}from\"@modelcontextprotocol/sdk/client/index.js\";\nimport{StdioClientTransport}from\"@modelcontextprotocol/sdk/client/stdio.js\";\n\n// Create server parameters for stdio connection\nconstserverParams=newStdioClientTransport({\ncommand:\"npx\",// Executable\nargs:[\"-y\",\"@philschmid/weather-mcp\"]// MCP Server\n});\n\nconstclient=newClient(\n{\nname:\"example-client\",\nversion:\"1.0.0\"\n}\n);\n\n// Configure the client\nconstai=newGoogleGenAI({});\n\n// Initialize the connection between client and server\nawaitclient.connect(serverParams);\n\n// Send request to the model with MCP tools\nconstresponse=awaitai.models.generateContent({\nmodel:\"gemini-2.5-flash\",\ncontents:`What is the weather in London in ${newDate().toLocaleDateString()}?`,\nconfig:{\ntools:[mcpToTool(client)],// uses the session, will automatically call the tool\n// Uncomment if you **don't** want the sdk to automatically call the tool\n// automaticFunctionCalling: {\n//   disable: true,\n// },\n},\n});\nconsole.log(response.text)\n\n// Close the connection\nawaitclient.close();\n\n```\n\n### Limitations with built-in MCP support\nBuilt-in MCP support is a [experimental](https://ai.google.dev/gemini-api/docs/models#preview) feature in our SDKs and has the following limitations:\n  * Only tools are supported, not resources nor prompts\n  * It is available for the Python and JavaScript/TypeScript SDK.\n  * Breaking changes might occur in future releases.\n\n\nManual integration of MCP servers is always an option if these limit what you're building.\n## Supported models\nThis section lists models and their function calling capabilities. Experimental models are not included. You can find a comprehensive capabilities overview on the [model overview](https://ai.google.dev/gemini-api/docs/models) page.\nModel | Function Calling | Parallel Function Calling | Compositional Function Calling  \n---|---|---|---  \nGemini 3 Pro | ✔️ | ✔️ | ✔️  \nGemini 3 Flash | ✔️ | ✔️ | ✔️  \nGemini 2.5 Pro | ✔️ | ✔️ | ✔️  \nGemini 2.5 Flash | ✔️ | ✔️ | ✔️  \nGemini 2.5 Flash-Lite | ✔️ | ✔️ | ✔️  \nGemini 2.0 Flash | ✔️ | ✔️ | ✔️  \nGemini 2.0 Flash-Lite | X | X | X  \n## Best practices\n  * **Function and Parameter Descriptions:** Be extremely clear and specific in your descriptions. The model relies on these to choose the correct function and provide appropriate arguments.\n  * **Naming:** Use descriptive function names (without spaces, periods, or dashes).\n  * **Strong Typing:** Use specific types (integer, string, enum) for parameters to reduce errors. If a parameter has a limited set of valid values, use an enum.\n  * **Tool Selection:** While the model can use an arbitrary number of tools, providing too many can increase the risk of selecting an incorrect or suboptimal tool. For best results, aim to provide only the relevant tools for the context or task, ideally keeping the active set to a maximum of 10-20. Consider dynamic tool selection based on conversation context if you have a large total number of tools.\n  * **Prompt Engineering:**\n    * Provide context: Tell the model its role (e.g., \"You are a helpful weather assistant.\").\n    * Give instructions: Specify how and when to use functions (e.g., \"Don't guess dates; always use a future date for forecasts.\").\n    * Encourage clarification: Instruct the model to ask clarifying questions if needed.\n    * See [Agentic workflows](https://ai.google.dev/gemini-api/docs/prompting-strategies#agentic-workflows) for further strategies on designing these prompts. Here is an example of a tested [system instruction](https://ai.google.dev/gemini-api/docs/prompting-strategies#agentic-si-template).\n  * **Temperature:** Use a low temperature (e.g., 0) for more deterministic and reliable function calls.\n  * **Validation:** If a function call has significant consequences (e.g., placing an order), validate the call with the user before executing it.\n  * **Check Finish Reason:** Always check the [`finishReason`](https://ai.google.dev/api/generate-content#FinishReason) in the model's response to handle cases where the model failed to generate a valid function call.\n  * **Error Handling** : Implement robust error handling in your functions to gracefully handle unexpected inputs or API failures. Return informative error messages that the model can use to generate helpful responses to the user.\n  * **Security:** Be mindful of security when calling external APIs. Use appropriate authentication and authorization mechanisms. Avoid exposing sensitive data in function calls.\n  * **Token Limits:** Function descriptions and parameters count towards your input token limit. If you're hitting token limits, consider limiting the number of functions or the length of the descriptions, break down complex tasks into smaller, more focused function sets.\n\n\n## Notes and limitations\n  * Only a [subset of the OpenAPI schema](https://ai.google.dev/api/caching#FunctionDeclaration) is supported.\n  * For `ANY` mode, the API may reject very large or deeply nested schemas. If you encounter errors, try simplifying your function parameter and response schemas by shortening property names, reducing nesting, or limiting the number of function declarations.\n  * Supported parameter types in Python are limited.\n  * Automatic function calling is a Python SDK feature only.\n\n\nSend feedback \nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2026-02-12 UTC.\n",
  "success": true
}
]